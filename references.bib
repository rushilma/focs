@online{alaouiInformationTheoreticViewStochastic2021,
  title = {An {{Information-Theoretic View}} of {{Stochastic Localization}}},
  author = {Alaoui, Ahmed El and Montanari, Andrea},
  date = {2021-09-09},
  eprint = {2109.00709},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2109.00709},
  url = {http://arxiv.org/abs/2109.00709},
  urldate = {2024-10-28},
  abstract = {Given a probability measure \$\textbackslash mu\$ over \$\{\textbackslash mathbb R\}\textasciicircum n\$, it is often useful to approximate it by the convex combination of a small number of probability measures, such that each component is close to a product measure. Recently, Ronen Eldan used a stochastic localization argument to prove a general decomposition result of this type. In Eldan's theorem, the `number of components' is characterized by the entropy of the mixture, and `closeness to product' is characterized by the covariance matrix of each component. We present an elementary proof of Eldan's theorem which makes use of an information theory (or estimation theory) interpretation. The proof is analogous to the one of an earlier decomposition result known as the `pinning lemma.'},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Mathematics - Information Theory,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/NZRSHT2H/Alaoui and Montanari - 2021 - An Information-Theoretic View of Stochastic Localization.pdf;/Users/rushilma/Zotero/storage/SJTZXQVC/2109.html}
}

@online{alaouiSamplingSherringtonKirkpatrickGibbs2024,
  title = {Sampling from the {{Sherrington-Kirkpatrick Gibbs}} Measure via Algorithmic Stochastic Localization},
  author = {Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
  date = {2024-02-15},
  eprint = {2203.05093},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2203.05093},
  url = {http://arxiv.org/abs/2203.05093},
  urldate = {2024-10-28},
  abstract = {We consider the Sherrington-Kirkpatrick model of spin glasses at high-temperature and no external field, and study the problem of sampling from the Gibbs distribution \$\textbackslash mu\$ in polynomial time. We prove that, for any inverse temperature \$\textbackslash beta{$<$}1/2\$, there exists an algorithm with complexity \$O(n\textasciicircum 2)\$ that samples from a distribution \$\textbackslash mu\textasciicircum\{alg\}\$ which is close in normalized Wasserstein distance to \$\textbackslash mu\$. Namely, there exists a coupling of \$\textbackslash mu\$ and \$\textbackslash mu\textasciicircum\{alg\}\$ such that if \$(x,x\textasciicircum\{alg\})\textbackslash in\textbackslash\{-1,+1\textbackslash\}\textasciicircum n\textbackslash times \textbackslash\{-1,+1\textbackslash\}\textasciicircum n\$ is a pair drawn from this coupling, then \$n\textasciicircum\{-1\}\textbackslash mathbb E\textbackslash\{||x-x\textasciicircum\{alg\}||\_2\textasciicircum 2\textbackslash\}=o\_n(1)\$. The best previous results, by Bauerschmidt and Bodineau and by Eldan, Koehler, and Zeitouni, implied efficient algorithms to approximately sample (under a stronger metric) for \$\textbackslash beta{$<$}1/4\$. We complement this result with a negative one, by introducing a suitable "stability" property for sampling algorithms, which is verified by many standard techniques. We prove that no stable algorithm can approximately sample for \$\textbackslash beta{$>$}1\$, even under the normalized Wasserstein metric. Our sampling method is based on an algorithmic implementation of stochastic localization, which progressively tilts the measure \$\textbackslash mu\$ towards a single configuration, together with an approximate message passing algorithm that is used to approximate the mean of the tilted measure.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/PDM9RGQL/Alaoui et al. - 2024 - Sampling from the Sherrington-Kirkpatrick Gibbs measure via algorithmic stochastic localization.pdf;/Users/rushilma/Zotero/storage/MEP6G28L/2203.html}
}

@article{bovierAlmostSureLarge1996,
  title = {An Almost Sure Large Deviation Principle for the {{Hopfield}} Model},
  author = {Bovier, Anton and Gayrard, Véronique},
  date = {1996-07},
  journaltitle = {The Annals of Probability},
  volume = {24},
  number = {3},
  pages = {1444--1475},
  publisher = {Institute of Mathematical Statistics},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/aop/1065725188},
  url = {https://projecteuclid.org/journals/annals-of-probability/volume-24/issue-3/An-almost-sure-large-deviation-principle-for-the-Hopfield-model/10.1214/aop/1065725188.full},
  urldate = {2024-12-24},
  abstract = {We prove a large deviation principle for the finite-dimensional marginals of the Gibbs distribution of the macroscopic "overlap" parameters in the Hopfield model in the case where the number of random "patterns" M , as a function of the system size N, satisfies lim sup \$M(N) /N =0\$. In this case, the rate function is independent of the disorder for almost all realizations of the patterns.},
  keywords = {60F10,82B44,82C32,Hopfield model,large deviations,neural networks,Self-averaging},
  file = {/Users/rushilma/Zotero/storage/DBJJ46F8/Bovier and Gayrard - 1996 - An almost sure large deviation principle for the Hopfield model.pdf}
}

@online{bubeckEntropicCLTPhase2018,
  title = {Entropic {{CLT}} and Phase Transition in High-Dimensional {{Wishart}} Matrices},
  author = {Bubeck, Sébastien and Ganguly, Shirshendu},
  date = {2018-08-12},
  eprint = {1509.03258},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1509.03258},
  url = {http://arxiv.org/abs/1509.03258},
  urldate = {2024-12-24},
  abstract = {We consider high dimensional Wishart matrices \$\textbackslash mathbb\{X\} \textbackslash mathbb\{X\}\textasciicircum\{\textbackslash top\}\$ where the entries of \$\textbackslash mathbb\{X\} \textbackslash in \{\textbackslash mathbb\{R\}\textasciicircum\{n \textbackslash times d\}\}\$ are i.i.d. from a log-concave distribution. We prove an information theoretic phase transition: such matrices are close in total variation distance to the corresponding Gaussian ensemble if and only if \$d\$ is much larger than \$n\textasciicircum 3\$. Our proof is entropy-based, making use of the chain rule for relative entropy along with the recursive structure in the definition of the Wishart ensemble. The proof crucially relies on the well known relation between Fisher information and entropy, a variational representation for Fisher information, concentration bounds for the spectral norm of a random matrix, and certain small ball probability estimates for log-concave measures.},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Mathematics - Functional Analysis,Mathematics - Information Theory,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/I2CEDXSB/Bubeck and Ganguly - 2018 - Entropic CLT and phase transition in high-dimensional Wishart matrices.pdf;/Users/rushilma/Zotero/storage/C7KPJSIX/1509.html}
}

@online{cardoneEntropicCLTOrder2022,
  title = {Entropic {{CLT}} for {{Order Statistics}}},
  author = {Cardone, Martina and Dytso, Alex and Rush, Cynthia},
  date = {2022-05-10},
  eprint = {2205.04621},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.04621},
  url = {http://arxiv.org/abs/2205.04621},
  urldate = {2024-12-28},
  abstract = {It is well known that central order statistics exhibit a central limit behavior and converge to a Gaussian distribution as the sample size grows. This paper strengthens this known result by establishing an entropic version of the CLT that ensures a stronger mode of convergence using the relative entropy. In particular, an order \$O(1/\textbackslash sqrt\{n\})\$ rate of convergence is established under mild conditions on the parent distribution of the sample generating the order statistics. To prove this result, ancillary results on order statistics are derived, which might be of independent interest.},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Mathematics - Information Theory,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/YSMQ5C8V/Cardone et al. - 2022 - Entropic CLT for Order Statistics.pdf;/Users/rushilma/Zotero/storage/Y7I4D67W/2205.html}
}

@online{chatterjeeShortSurveySteins2014,
  title = {A Short Survey of {{Stein}}'s Method},
  author = {Chatterjee, Sourav},
  date = {2014-04-04},
  eprint = {1404.1392},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1404.1392},
  url = {http://arxiv.org/abs/1404.1392},
  urldate = {2025-01-02},
  abstract = {Stein's method is a powerful technique for proving central limit theorems in probability theory when more straightforward approaches cannot be implemented easily. This article begins with a survey of the historical development of Stein's method and some recent advances. This is followed by a description of a "general purpose" variant of Stein's method that may be called the generalized perturbative approach, and an application of this method to minimal spanning trees. The article concludes with the descriptions of some well known open problems that may possibly be solved by the perturbative approach or some other variant of Stein's method.},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/LRCYUV73/Chatterjee - 2014 - A short survey of Stein's method.pdf;/Users/rushilma/Zotero/storage/EQ7S2EMF/1404.html}
}

@online{donohoMessagePassingAlgorithms2009,
  title = {Message {{Passing Algorithms}} for {{Compressed Sensing}}},
  author = {Donoho, David L. and Maleki, Arian and Montanari, Andrea},
  date = {2009-07-21},
  eprint = {0907.3574},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.0907.3574},
  url = {http://arxiv.org/abs/0907.3574},
  urldate = {2024-10-28},
  abstract = {Compressed sensing aims to undersample certain high-dimensional signals, yet accurately reconstruct them by exploiting signal characteristics. Accurate reconstruction is possible when the object to be recovered is sufficiently sparse in a known basis. Currently, the best known sparsity-undersampling tradeoff is achieved when reconstructing by convex optimization -- which is expensive in important large-scale applications. Fast iterative thresholding algorithms have been intensively studied as alternatives to convex optimization for large-scale problems. Unfortunately known fast algorithms offer substantially worse sparsity-undersampling tradeoffs than convex optimization. We introduce a simple costless modification to iterative thresholding making the sparsity-undersampling tradeoff of the new algorithms equivalent to that of the corresponding convex optimization procedures. The new iterative-thresholding algorithms are inspired by belief propagation in graphical models. Our empirical measurements of the sparsity-undersampling tradeoff for the new algorithms agree with theoretical calculations. We show that a state evolution formalism correctly derives the true sparsity-undersampling tradeoff. There is a surprising agreement between earlier calculations based on random convex polytopes and this new, apparently very different theoretical formalism.},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Information Theory,Statistics - Computation},
  file = {/Users/rushilma/Zotero/storage/9PRPR8W4/Donoho et al. - 2009 - Message Passing Algorithms for Compressed Sensing.pdf;/Users/rushilma/Zotero/storage/998GYA42/0907.html}
}

@online{eldanCLTHighDimensions2020,
  title = {The {{CLT}} in High Dimensions: Quantitative Bounds via Martingale Embedding},
  shorttitle = {The {{CLT}} in High Dimensions},
  author = {Eldan, Ronen and Mikulincer, Dan and Zhai, Alex},
  date = {2020-09-07},
  eprint = {1806.09087},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1806.09087},
  url = {http://arxiv.org/abs/1806.09087},
  urldate = {2024-12-28},
  abstract = {We introduce a new method for obtaining quantitative convergence rates for the central limit theorem (CLT) in a high dimensional setting. Using our method, we obtain several new bounds for convergence in transportation distance and entropy, and in particular: (a) We improve the best known bound, obtained by the third named author, for convergence in quadratic Wasserstein transportation distance for bounded random vectors; (b) We derive the first non-asymptotic convergence rate for the entropic CLT in arbitrary dimension, for general log-concave random vectors; (c) We give an improved bound for convergence in transportation distance under a log-concavity assumption and improvements for both metrics under the assumption of strong log-concavity. Our method is based on martingale embeddings and specifically on the Skorokhod embedding constructed by the first named author.},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/92XNVPB6/Eldan et al. - 2020 - The CLT in high dimensions quantitative bounds via martingale embedding.pdf;/Users/rushilma/Zotero/storage/IYV3FD4C/1806.html}
}

@online{gamarnikAlgorithmicObstructionsRandom2021,
  title = {Algorithmic {{Obstructions}} in the {{Random Number Partitioning Problem}}},
  author = {Gamarnik, David and Kızıldağ, Eren C.},
  date = {2021-03-02},
  eprint = {2103.01369},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2103.01369},
  url = {http://arxiv.org/abs/2103.01369},
  urldate = {2025-01-17},
  abstract = {We consider the algorithmic problem of finding a near-optimal solution for the number partitioning problem (NPP). The NPP appears in many applications, including the design of randomized controlled trials, multiprocessor scheduling, and cryptography; and is also of theoretical significance. It possesses a so-called statistical-to-computational gap: when its input \$X\$ has distribution \$\textbackslash mathcal\{N\}(0,I\_n)\$, its optimal value is \$\textbackslash Theta(\textbackslash sqrt\{n\}2\textasciicircum\{-n\})\$ w.h.p.; whereas the best polynomial-time algorithm achieves an objective value of only \$2\textasciicircum\{-\textbackslash Theta(\textbackslash log\textasciicircum 2 n)\}\$, w.h.p. In this paper, we initiate the study of the nature of this gap. Inspired by insights from statistical physics, we study the landscape of NPP and establish the presence of the Overlap Gap Property (OGP), an intricate geometric property which is known to be a rigorous evidence of an algorithmic hardness for large classes of algorithms. By leveraging the OGP, we establish that (a) any sufficiently stable algorithm, appropriately defined, fails to find a near-optimal solution with energy below \$2\textasciicircum\{-\textbackslash omega(n \textbackslash log\textasciicircum\{-1/5\} n)\}\$; and (b) a very natural MCMC dynamics fails to find near-optimal solutions. Our simulations suggest that the state of the art algorithm achieving \$2\textasciicircum\{-\textbackslash Theta(\textbackslash log\textasciicircum 2 n)\}\$ is indeed stable, but formally verifying this is left as an open problem. OGP regards the overlap structure of \$m-\$tuples of solutions achieving a certain objective value. When \$m\$ is constant we prove the presence of OGP in the regime \$2\textasciicircum\{-\textbackslash Theta(n)\}\$, and the absence of it in the regime \$2\textasciicircum\{-o(n)\}\$. Interestingly, though, by considering overlaps with growing values of \$m\$ we prove the presence of the OGP up to the level \$2\textasciicircum\{-\textbackslash omega(\textbackslash sqrt\{n\textbackslash log n\})\}\$. Our proof of the failure of stable algorithms at values \$2\textasciicircum\{-\textbackslash omega(n \textbackslash log\textasciicircum\{-1/5\} n)\}\$ employs methods from Ramsey Theory from the extremal combinatorics, and is of independent interest.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/Y7WERI6E/Gamarnik and Kızıldağ - 2021 - Algorithmic Obstructions in the Random Number Partitioning Problem.pdf}
}

@article{gamarnikOverlapGapProperty2021,
  title = {The {{Overlap Gap Property}}: A {{Geometric Barrier}} to {{Optimizing}} over {{Random Structures}}},
  shorttitle = {The {{Overlap Gap Property}}},
  author = {Gamarnik, David},
  date = {2021-10-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {118},
  number = {41},
  eprint = {2109.14409},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {e2108492118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2108492118},
  url = {http://arxiv.org/abs/2109.14409},
  urldate = {2025-01-17},
  abstract = {The problem of optimizing over random structures emerges in many areas of science and engineering, ranging from statistical physics to machine learning and artificial intelligence. For many such structures finding optimal solutions by means of fast algorithms is not known and often is believed not possible. At the same time the formal hardness of these problems in form of say complexity-theoretic N P -hardness is lacking.},
  langid = {english},
  keywords = {Computer Science - Computational Complexity,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/EFY7NBSI/Gamarnik - 2021 - The Overlap Gap Property a Geometric Barrier to Optimizing over Random Structures.pdf}
}

@online{greniouxStochasticLocalizationIterative2024,
  title = {Stochastic {{Localization}} via {{Iterative Posterior Sampling}}},
  author = {Grenioux, Louis and Noble, Maxence and Gabrié, Marylou and Durmus, Alain Oliviero},
  date = {2024-05-28},
  eprint = {2402.10758},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2402.10758},
  url = {http://arxiv.org/abs/2402.10758},
  urldate = {2024-12-28},
  abstract = {Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, \$\textbackslash textit\{Stochastic Localization via Iterative Posterior Sampling\}\$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several benchmarks of multi-modal distributions, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/8SAUQGLG/Grenioux et al. - 2024 - Stochastic Localization via Iterative Posterior Sampling.pdf;/Users/rushilma/Zotero/storage/SU3L3GKC/2402.html}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-0-387-84858-7},
  url = {http://link.springer.com/10.1007/978-0-387-84858-7},
  urldate = {2024-12-28},
  isbn = {978-0-387-84857-0 978-0-387-84858-7}
}

@online{montanariProvablyEfficientPosterior2024,
  title = {Provably {{Efficient Posterior Sampling}} for {{Sparse Linear Regression}} via {{Measure Decomposition}}},
  author = {Montanari, Andrea and Wu, Yuchen},
  date = {2024-06-27},
  eprint = {2406.19550},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2406.19550},
  url = {http://arxiv.org/abs/2406.19550},
  urldate = {2024-10-28},
  abstract = {We consider the problem of sampling from the posterior distribution of a \$d\$-dimensional coefficient vector \$\textbackslash boldsymbol\{\textbackslash theta\}\$, given linear observations \$\textbackslash boldsymbol\{y\} = \textbackslash boldsymbol\{X\}\textbackslash boldsymbol\{\textbackslash theta\}+\textbackslash boldsymbol\{\textbackslash varepsilon\}\$. In general, such posteriors are multimodal, and therefore challenging to sample from. This observation has prompted the exploration of various heuristics that aim at approximating the posterior distribution. In this paper, we study a different approach based on decomposing the posterior distribution into a log-concave mixture of simple product measures. This decomposition allows us to reduce sampling from a multimodal distribution of interest to sampling from a log-concave one, which is tractable and has been investigated in detail. We prove that, under mild conditions on the prior, for random designs, such measure decomposition is generally feasible when the number of samples per parameter \$n/d\$ exceeds a constant threshold. We thus obtain a provably efficient (polynomial time) sampling algorithm in a regime where this was previously not known. Numerical simulations confirm that the algorithm is practical, and reveal that it has attractive statistical properties compared to state-of-the-art methods.},
  pubstate = {prepublished},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/AKTM5KZI/Montanari and Wu - 2024 - Provably Efficient Posterior Sampling for Sparse Linear Regression via Measure Decomposition.pdf;/Users/rushilma/Zotero/storage/HWWJMKNH/2406.html}
}

@book{wainwrightHighDimensionalStatisticsNonAsymptotic2019,
  title = {High-{{Dimensional Statistics}}: {{A Non-Asymptotic Viewpoint}}},
  shorttitle = {High-{{Dimensional Statistics}}},
  author = {Wainwright, Martin J.},
  date = {2019},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/9781108627771},
  url = {https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E},
  urldate = {2024-12-28},
  abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
  isbn = {978-1-108-49802-9},
  file = {/Users/rushilma/Zotero/storage/E4LYV3IK/8A91ECEEC38F46DAB53E9FF8757C7A4E.html}
}

@article{zhaoHopfieldModelSuperlinearly2012,
  title = {The {{Hopfield Model}} with {{Superlinearly Many Patterns}}},
  author = {Zhao, James Y.},
  date = {2012-10-06},
  journaltitle = {Statistics \& Probability Letters},
  shortjournal = {Statistics \& Probability Letters},
  volume = {83},
  number = {1},
  eprint = {1108.4771},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {350--356},
  issn = {01677152},
  doi = {10.1016/j.spl.2012.09.026},
  url = {http://arxiv.org/abs/1108.4771},
  urldate = {2024-12-24},
  abstract = {We study the Hopfield model where the ratio \$\textbackslash alpha\$ of patterns to sites grows large. We prove that the free energy with inverse temperature \$\textbackslash beta\$ and external field \$B\$ behaves like \$\textbackslash beta\textbackslash sqrt\textbackslash alpha+\textbackslash gamma\$, where \$\textbackslash gamma\$ is the limiting free energy of the Sherrington-Kirkpatrick model with inverse temperature \$\textbackslash sqrt2\textbackslash beta\$ and external field \$B\$.},
  keywords = {Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/3PN2T3TU/Zhao - 2013 - The Hopfield Model with Superlinearly Many Patterns.pdf;/Users/rushilma/Zotero/storage/H487A9GG/1108.html}
}

@online{zouConciseTutorialApproximate2022,
  title = {A {{Concise Tutorial}} on {{Approximate Message Passing}}},
  author = {Zou, Qiuyun and Yang, Hongwen},
  date = {2022-03-01},
  eprint = {2201.07487},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2201.07487},
  url = {http://arxiv.org/abs/2201.07487},
  urldate = {2024-10-28},
  abstract = {High-dimensional signal recovery of standard linear regression is a key challenge in many engineering fields, such as, communications, compressed sensing, and image processing. The approximate message passing (AMP) algorithm proposed by Donoho \textbackslash textit\{et al\} is a computational efficient method to such problems, which can attain Bayes-optimal performance in independent identical distributed (IID) sub-Gaussian random matrices region. A significant feature of AMP is that the dynamical behavior of AMP can be fully predicted by a scalar equation termed station evolution (SE). Although AMP is optimal in IID sub-Gaussian random matrices, AMP may fail to converge when measurement matrix is beyond IID sub-Gaussian. To extend the region of random measurement matrix, an expectation propagation (EP)-related algorithm orthogonal AMP (OAMP) was proposed, which shares the same algorithm with EP, expectation consistent (EC), and vector AMP (VAMP). This paper aims at giving a review for those algorithms. We begin with the worst case, i.e., least absolute shrinkage and selection operator (LASSO) inference problem, and then give the detailed derivation of AMP derived from message passing. Also, in the Bayes-optimal setting, we give the Bayes-optimal AMP which has a slight difference from AMP for LASSO. In addition, we review some AMP-related algorithms: OAMP, VAMP, and Memory AMP (MAMP), which can be applied to more general random matrices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Mathematics - Information Theory},
  file = {/Users/rushilma/Zotero/storage/MIQSW8WP/Zou and Yang - 2022 - A Concise Tutorial on Approximate Message Passing.pdf;/Users/rushilma/Zotero/storage/JXPJKVW6/2201.html}
}
