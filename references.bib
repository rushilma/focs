@inproceedings{achlioptasAlgorithmicBarriersPhase2008,
  title = {Algorithmic Barriers from Phase Transitions},
  booktitle = {2008 49th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Achlioptas, Dimitris and Coja-Oghlan, Amin},
  date = {2008-10},
  eprint = {0803.2122},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {793--802},
  doi = {10.1109/FOCS.2008.11},
  url = {http://arxiv.org/abs/0803.2122},
  urldate = {2025-03-16},
  abstract = {For many random Constraint Satisfaction Problems, by now, we have asymptotically tight estimates of the largest constraint density for which they have solutions. At the same time, all known polynomial-time algorithms for many of these problems already completely fail to find solutions at much smaller densities. For example, it is well-known that it is easy to color a random graph using twice as many colors as its chromatic number. Indeed, some of the simplest possible coloring algorithms already achieve this goal. Given the simplicity of those algorithms, one would expect there is a lot of room for improvement. Yet, to date, no algorithm is known that uses \$(2-\textbackslash epsilon) \textbackslash chi\$ colors, in spite of efforts by numerous researchers over the years. In view of the remarkable resilience of this factor of 2 against every algorithm hurled at it, we believe it is natural to inquire into its origin. We do so by analyzing the evolution of the set of \$k\$-colorings of a random graph, viewed as a subset of \$\textbackslash\{1,...,k\textbackslash\}\textasciicircum\{n\}\$, as edges are added. We prove that the factor of 2 corresponds in a precise mathematical sense to a phase transition in the geometry of this set. Roughly, the set of \$k\$-colorings looks like a giant ball for \$k \textbackslash ge 2 \textbackslash chi\$, but like an error-correcting code for \$k \textbackslash le (2-\textbackslash epsilon) \textbackslash chi\$. We prove that a completely analogous phase transition also occurs both in random \$k\$-SAT and in random hypergraph 2-coloring. And that for each problem, its location corresponds precisely with the point were all known polynomial-time algorithms fail. To prove our results we develop a general technique that allows us to prove rigorously much of the celebrated 1-step Replica-Symmetry-Breaking hypothesis of statistical physics for random CSPs.},
  keywords = {Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/5K862VS5/Achlioptas and Coja-Oghlan - 2008 - Algorithmic barriers from phase transitions.pdf;/Users/rushilma/Zotero/storage/RWGDN4NG/0803.html}
}

@online{achlioptasSolutionSpaceGeometryRandom2006,
  title = {On the {{Solution-Space Geometry}} of {{Random Constraint Satisfaction Problems}}},
  author = {Achlioptas, Dimitris and Ricci-Tersenghi, Federico},
  date = {2006-12-15},
  eprint = {cs/0611052},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.cs/0611052},
  url = {http://arxiv.org/abs/cs/0611052},
  urldate = {2025-03-16},
  abstract = {For a large number of random constraint satisfaction problems, such as random k-SAT and random graph and hypergraph coloring, there are very good estimates of the largest constraint density for which solutions exist. Yet, all known polynomial-time algorithms for these problems fail to find solutions even at much lower densities. To understand the origin of this gap we study how the structure of the space of solutions evolves in such problems as constraints are added. In particular, we prove that much before solutions disappear, they organize into an exponential number of clusters, each of which is relatively small and far apart from all other clusters. Moreover, inside each cluster most variables are frozen, i.e., take only one value. The existence of such frozen variables gives a satisfying intuitive explanation for the failure of the polynomial-time algorithms analyzed so far. At the same time, our results establish rigorously one of the two main hypotheses underlying Survey Propagation, a heuristic introduced by physicists in recent years that appears to perform extraordinarily well on random constraint satisfaction problems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Condensed Matter - Disordered Systems and Neural Networks},
  file = {/Users/rushilma/Zotero/storage/IFMXSYCX/Achlioptas and Ricci-Tersenghi - 2006 - On the Solution-Space Geometry of Random Constraint Satisfaction Problems.pdf;/Users/rushilma/Zotero/storage/TBMAZ68B/0611052.html}
}

@online{addario-berryLocalOptimaSherringtonKirkpatrick2017,
  title = {Local Optima of the {{Sherrington-Kirkpatrick Hamiltonian}}},
  author = {Addario-Berry, Louigi and Devroye, Luc and Lugosi, Gabor and Oliveira, Roberto Imbuzeiro},
  date = {2017-12-21},
  eprint = {1712.07775},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1712.07775},
  url = {http://arxiv.org/abs/1712.07775},
  urldate = {2025-03-16},
  abstract = {We study local optima of the Hamiltonian of the Sherrington-Kirkpatrick model. We compute the exponent of the expected number of local optima and determine the "typical" value of the Hamiltonian.},
  pubstate = {prepublished},
  keywords = {Computer Science - Discrete Mathematics,Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/KBJXB23U/Addario-Berry et al. - 2017 - Local optima of the Sherrington-Kirkpatrick Hamiltonian.pdf;/Users/rushilma/Zotero/storage/8Q4H528T/1712.html}
}

@article{alidaeeNewModelingSolution2005,
  title = {A New Modeling and Solution Approach for the Number Partitioning Problem},
  author = {Alidaee, Bahram and Glover, Fred and Kochenberger, Gary A. and Rego, Cesar},
  date = {2005-01-01},
  journaltitle = {Journal of Applied Mathematics and Decision Sciences},
  shortjournal = {Journal of Applied Mathematics and Decision Sciences},
  volume = {2005},
  number = {2},
  pages = {113--121},
  issn = {1173-9126, 1532-7612},
  doi = {10.1155/JAMDS.2005.113},
  url = {https://www.hindawi.com/journals/ads/2005/317849/abs/},
  urldate = {2025-03-15},
  abstract = {The number partitioning problem has proven to be a challenging problem for both exact and heuristic solution methods. We present a new modeling and solution approach that consists of recasting the problem as an unconstrained quadratic binary program that can be solved by efficient metaheuristic methods. Our approach readily accommodates both the common two-subset partition case as well as the more general case of multiple subsets. Preliminary computational experience is presented illustrating the attractiveness of the method.},
  langid = {english}
}

@article{arguelloRandomizedMethodsNumber1996,
  title = {Randomized Methods for the Number Partitioning Problem},
  author = {Argüello, Michael F. and Feo, Thomas A. and Goldschmidt, Olivier},
  date = {1996-02-01},
  journaltitle = {Computers \& Operations Research},
  shortjournal = {Computers \& Operations Research},
  volume = {23},
  number = {2},
  pages = {103--111},
  issn = {0305-0548},
  doi = {10.1016/0305-0548(95)E0020-L},
  url = {https://www.sciencedirect.com/science/article/pii/0305054895E0020L},
  urldate = {2025-03-15},
  abstract = {Randomized versions of Karmarkar and Karp's differencing method are introduced for the Number Partitioning problem. The development of these methods and a discussion of their merits are presented. It is shown that these randomized heuristics consistently yield better solutions than those generated by the differencing method.}
}

@article{asproniAccuracyMinorEmbedding2020,
  title = {Accuracy and Minor Embedding in Subqubo Decomposition with Fully Connected Large Problems: A Case Study about the Number Partitioning Problem},
  shorttitle = {Accuracy and Minor Embedding in Subqubo Decomposition with Fully Connected Large Problems},
  author = {Asproni, Luca and Caputo, Davide and Silva, Blanca and Fazzi, Giovanni and Magagnini, Marco},
  date = {2020-06},
  journaltitle = {Quantum Machine Intelligence},
  shortjournal = {Quantum Mach. Intell.},
  volume = {2},
  number = {1},
  pages = {4},
  issn = {2524-4906, 2524-4914},
  doi = {10.1007/s42484-020-00014-w},
  url = {http://link.springer.com/10.1007/s42484-020-00014-w},
  urldate = {2025-03-15},
  abstract = {Abstract                            In this work, we investigate the capabilities of a hybrid quantum-classical procedure to explore the solution space using the D-Wave 2000               Q                                T                 M                              quantum annealer device. Here, we study the ability of the quantum hardware to solve the number partitioning problem, a well-known NP-hard optimization model that poses some challenges typical of those encountered in real-world applications. This represents one of the most complex scenario in terms of qubits connectivity and, by increasing the input problem size, we analyze the scaling properties of the quantum-classical workflow. We find remarkable results in most instances of the model; for the most complex ones, we investigate further the D-Wave Hybrid suite. Specifically, we were able to find the optimal solutions even in the worst cases by fine-tuning the parameters that schedule the annealing time and allowing a pause in the annealing cycle.},
  langid = {english},
  file = {/Users/rushilma/Zotero/storage/MR9I3YHX/Asproni et al. - 2020 - Accuracy and minor embedding in subqubo decomposition with fully connected large problems a case st.pdf}
}

@article{aubinStorageCapacitySymmetric2019,
  title = {Storage Capacity in Symmetric Binary Perceptrons},
  author = {Aubin, Benjamin and Perkins, Will and Zdeborová, Lenka},
  date = {2019-07-19},
  journaltitle = {Journal of Physics A: Mathematical and Theoretical},
  shortjournal = {J. Phys. A: Math. Theor.},
  volume = {52},
  number = {29},
  eprint = {1901.00314},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  pages = {294003},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8121/ab227a},
  url = {http://arxiv.org/abs/1901.00314},
  urldate = {2025-03-16},
  abstract = {We study the problem of determining the capacity of the binary perceptron for two variants of the problem where the corresponding constraint is symmetric. We call these variants the rectangle-binary-perceptron (RPB) and the \$u-\$function-binary-perceptron (UBP). We show that, unlike for the usual step-function-binary-perceptron, the critical capacity in these symmetric cases is given by the annealed computation in a large region of parameter space (for all rectangular constraints and for narrow enough \$u-\$function constraints, \$K{$<$}K\textasciicircum *\$). We prove this fact (under two natural assumptions) using the first and second moment methods. We further use the second moment method to conjecture that solutions of the symmetric binary perceptrons are organized in a so-called frozen-1RSB structure, without using the replica method. We then use the replica method to estimate the capacity threshold for the UBP case when the \$u-\$function is wide \$K{$>$}K\textasciicircum *\$. We conclude that full-step-replica-symmetry breaking would have to be evaluated in order to obtain the exact capacity in this case.},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/T6IESM3A/Aubin et al. - 2019 - Storage capacity in symmetric binary perceptrons.pdf;/Users/rushilma/Zotero/storage/VRGHGP9K/1901.html}
}

@online{bandeiraNotesComputationaltostatisticalGaps2018,
  title = {Notes on Computational-to-Statistical Gaps: Predictions Using Statistical Physics},
  shorttitle = {Notes on Computational-to-Statistical Gaps},
  author = {Bandeira, Afonso S. and Perry, Amelia and Wein, Alexander S.},
  date = {2018-04-20},
  eprint = {1803.11132},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1803.11132},
  url = {http://arxiv.org/abs/1803.11132},
  urldate = {2025-03-16},
  abstract = {In these notes we describe heuristics to predict computational-to-statistical gaps in certain statistical problems. These are regimes in which the underlying statistical problem is information-theoretically possible although no efficient algorithm exists, rendering the problem essentially unsolvable for large instances. The methods we describe here are based on mature, albeit non-rigorous, tools from statistical physics. These notes are based on a lecture series given by the authors at the Courant Institute of Mathematical Sciences in New York City, on May 16th, 2017.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/H4FRMITX/Bandeira et al. - 2018 - Notes on computational-to-statistical gaps predictions using statistical physics.pdf;/Users/rushilma/Zotero/storage/MC9GN7D8/1803.html}
}

@online{bansalConstructiveAlgorithmsDiscrepancy2010,
  title = {Constructive {{Algorithms}} for {{Discrepancy Minimization}}},
  author = {Bansal, Nikhil},
  date = {2010-08-09},
  eprint = {1002.2259},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1002.2259},
  url = {http://arxiv.org/abs/1002.2259},
  urldate = {2025-03-16},
  abstract = {Given a set system (V,S), V=\{1,...,n\} and S=\{S1,...,Sm\}, the minimum discrepancy problem is to find a 2-coloring of V, such that each set is colored as evenly as possible. In this paper we give the first polynomial time algorithms for discrepancy minimization that achieve bounds similar to those known existentially using the so-called Entropy Method. We also give a first approximation-like result for discrepancy. The main idea in our algorithms is to produce a coloring over time by letting the color of the elements perform a random walk (with tiny increments) starting from 0 until they reach \$-1\$ or \$+1\$. At each time step the random hops for various elements are correlated using the solution to a semidefinite program, where this program is determined by the current state and the entropy method.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/rushilma/Zotero/storage/IU6UES78/Bansal - 2010 - Constructive Algorithms for Discrepancy Minimization.pdf;/Users/rushilma/Zotero/storage/9JSNQN6E/1002.html}
}

@online{barakNearlyTightSumofSquares2016,
  title = {A {{Nearly Tight Sum-of-Squares Lower Bound}} for the {{Planted Clique Problem}}},
  author = {Barak, Boaz and Hopkins, Samuel B. and Kelner, Jonathan and Kothari, Pravesh K. and Moitra, Ankur and Potechin, Aaron},
  date = {2016-04-12},
  eprint = {1604.03084},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1604.03084},
  url = {http://arxiv.org/abs/1604.03084},
  urldate = {2025-03-16},
  abstract = {We prove that with high probability over the choice of a random graph \$G\$ from the Erd\textbackslash H\{o\}s-R\textbackslash 'enyi distribution \$G(n,1/2)\$, the \$n\textasciicircum\{O(d)\}\$-time degree \$d\$ Sum-of-Squares semidefinite programming relaxation for the clique problem will give a value of at least \$n\textasciicircum\{1/2-c(d/\textbackslash log n)\textasciicircum\{1/2\}\}\$ for some constant \$c{$>$}0\$. This yields a nearly tight \$n\textasciicircum\{1/2 - o(1)\}\$ bound on the value of this program for any degree \$d = o(\textbackslash log n)\$. Moreover we introduce a new framework that we call \textbackslash emph\{pseudo-calibration\} to construct Sum of Squares lower bounds. This framework is inspired by taking a computational analog of Bayesian probability theory. It yields a general recipe for constructing good pseudo-distributions (i.e., dual certificates for the Sum-of-Squares semidefinite program), and sheds further light on the ways in which this hierarchy differs from others.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/rushilma/Zotero/storage/STVR5SCY/Barak et al. - 2016 - A Nearly Tight Sum-of-Squares Lower Bound for the Planted Clique Problem.pdf;/Users/rushilma/Zotero/storage/IDSMD88V/1604.html}
}

@article{baukeNumberPartitioningRandom2004,
  title = {Number Partitioning as a Random Energy Model},
  author = {Bauke, Heiko and Franz, Silvio and Mertens, Stephan},
  date = {2004-04},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2004},
  number = {04},
  pages = {P04003},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2004/04/P04003},
  url = {https://dx.doi.org/10.1088/1742-5468/2004/04/P04003},
  urldate = {2025-03-15},
  abstract = {Number partitioning is a classical problem from combinatorial optimization. In physical terms it corresponds to a long range anti-ferromagnetic Ising spin glass. It has been rigorously proven that the low lying energies of number partitioning behave like uncorrelated random variables. We claim that neighbouring energy levels are uncorrelated almost everywhere on the energy axis, and that energetically adjacent configurations are uncorrelated, too. Apparently there is no relation between geometry (configuration) and energy that could be exploited by an optimization algorithm. This ‘local random energy’ picture of number partitioning is corroborated by numerical simulations and heuristic arguments.},
  langid = {english},
  file = {/Users/rushilma/Zotero/storage/FUGTCS66/Bauke et al. - 2004 - Number partitioning as a random energy model.pdf}
}

@article{bayatiCombinatorialApproachInterpolation2013,
  title = {Combinatorial Approach to the Interpolation Method and Scaling Limits in Sparse Random Graphs},
  author = {Bayati, Mohsen and Gamarnik, David and Tetali, Prasad},
  date = {2013-11-01},
  journaltitle = {The Annals of Probability},
  shortjournal = {Ann. Probab.},
  volume = {41},
  number = {6},
  eprint = {0912.2444},
  eprinttype = {arXiv},
  eprintclass = {math},
  issn = {0091-1798},
  doi = {10.1214/12-AOP816},
  url = {http://arxiv.org/abs/0912.2444},
  urldate = {2025-03-16},
  abstract = {We establish the existence of free energy limits for several combinatorial models on Erd\textbackslash "\{o\}s-R\textbackslash '\{e\}nyi graph \$\textbackslash mathbb \{G\}(N,\textbackslash lfloor cN\textbackslash rfloor)\$ and random \$r\$-regular graph \$\textbackslash mathbb \{G\}(N,r)\$. For a variety of models, including independent sets, MAX-CUT, coloring and K-SAT, we prove that the free energy both at a positive and zero temperature, appropriately rescaled, converges to a limit as the size of the underlying graph diverges to infinity. In the zero temperature case, this is interpreted as the existence of the scaling limit for the corresponding combinatorial optimization problem. For example, as a special case we prove that the size of a largest independent set in these graphs, normalized by the number of nodes converges to a limit w.h.p. This resolves an open problem which was proposed by Aldous (Some open problems) as one of his six favorite open problems. It was also mentioned as an open problem in several other places: Conjecture 2.20 in Wormald [In Surveys in Combinatorics, 1999 (Canterbury) (1999) 239-298 Cambridge Univ. Press]; Bollob\textbackslash '\{a\}s and Riordan [Random Structures Algorithms 39 (2011) 1-38]; Janson and Thomason [Combin. Probab. Comput. 17 (2008) 259-264] and Aldous and Steele [In Probability on Discrete Structures (2004) 1-72 Springer].},
  keywords = {Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/RQEG99RX/Bayati et al. - 2013 - Combinatorial approach to the interpolation method and scaling limits in sparse random graphs.pdf;/Users/rushilma/Zotero/storage/JVFDQ5RC/0912.html}
}

@online{berthetComputationalLowerBounds2013,
  title = {Computational {{Lower Bounds}} for {{Sparse PCA}}},
  author = {Berthet, Quentin and Rigollet, Philippe},
  date = {2013-04-26},
  eprint = {1304.0828},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1304.0828},
  url = {http://arxiv.org/abs/1304.0828},
  urldate = {2025-03-16},
  abstract = {In the context of sparse principal component detection, we bring evidence towards the existence of a statistical price to pay for computational efficiency. We measure the performance of a test by the smallest signal strength that it can detect and we propose a computationally efficient method based on semidefinite programming. We also prove that the statistical performance of this test cannot be strictly improved by any computationally efficient method. Our results can be viewed as complexity theoretic lower bounds conditionally on the assumptions that some instances of the planted clique problem cannot be solved in randomized polynomial time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/BYR2NQEH/Berthet and Rigollet - 2013 - Computational Lower Bounds for Sparse PCA.pdf;/Users/rushilma/Zotero/storage/LYFVGFTR/1304.html}
}

@online{bismuthPartitioningProblemsSplittings2024,
  title = {Partitioning {{Problems}} with {{Splittings}} and {{Interval Targets}}},
  author = {Bismuth, Samuel and Makarov, Vladislav and Segal-Halevi, Erel and Shapira, Dana},
  date = {2024-09-18},
  eprint = {2204.11753},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.11753},
  url = {http://arxiv.org/abs/2204.11753},
  urldate = {2025-03-20},
  abstract = {The \$n\$-way number partitioning problem is a classic problem in combinatorial optimization, with applications to diverse settings such as fair allocation and machine scheduling. All these problems are NP-hard, but various approximation algorithms are known. We consider three closely related kinds of approximations. The first two variants optimize the partition such that: in the first variant some fixed number \$s\$ of items can be \textbackslash emph\{split\} between two or more bins and in the second variant we allow at most a fixed number \$t\$ of \textbackslash emph\{splittings\}. The third variant is a decision problem: the largest bin sum must be within a pre-specified interval, parameterized by a fixed rational number \$u\$ times the largest item size. When the number of bins \$n\$ is unbounded, we show that every variant is strongly \{\textbackslash sf NP\}-complete. When the number of bins \$n\$ is fixed, the running time depends on the fixed parameters \$s,t,u\$. For each variant, we give a complete picture of its running time. For \$n=2\$, the running time is easy to identify. Our main results consider any fixed integer \$n \textbackslash geq 3\$. Using a two-way polynomial-time reduction between the first and the third variant, we show that \$n\$-way number-partitioning with \$s\$ split items can be solved in polynomial time if \$s \textbackslash geq n-2\$, and it is \{\textbackslash sf NP\}-complete otherwise. Also, \$n\$-way number-partitioning with \$t\$ splittings can be solved in polynomial time if \$t \textbackslash geq n-1\$, and it is \{\textbackslash sf NP\}-complete otherwise. Finally, we show that the third variant can be solved in polynomial time if \$u \textbackslash geq (n-2)/n\$, and it is \{\textbackslash sf NP\}-complete otherwise. Our positive results for the optimization problems consider both min-max and max-min versions. Using the same reduction, we provide a fully polynomial-time approximation scheme for the case where the number of split items is lower than \$n-2\$.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms},
  file = {/Users/rushilma/Zotero/storage/AAP3ZJQ9/Bismuth et al. - 2024 - Partitioning Problems with Splittings and Interval Targets.pdf;/Users/rushilma/Zotero/storage/EZ7IYJ4Q/2204.html}
}

@article{boettcherAnalysisKarmarkarKarpDifferencing2008,
  title = {Analysis of the {{Karmarkar-Karp}} Differencing Algorithm},
  author = {Boettcher, S. and Mertens, S.},
  date = {2008-09},
  journaltitle = {The European Physical Journal B},
  shortjournal = {Eur. Phys. J. B},
  volume = {65},
  number = {1},
  pages = {131--140},
  issn = {1434-6028, 1434-6036},
  doi = {10.1140/epjb/e2008-00320-9},
  url = {http://link.springer.com/10.1140/epjb/e2008-00320-9},
  urldate = {2025-03-16},
  langid = {english}
}

@article{borgsPhaseTransitionFinitesize2001,
  title = {Phase Transition and Finite‐size Scaling for the Integer Partitioning Problem},
  author = {Borgs, Christian and Chayes, Jennifer and Pittel, Boris},
  date = {2001-10},
  journaltitle = {Random Structures \& Algorithms},
  shortjournal = {Random Struct Algorithms},
  volume = {19},
  number = {3--4},
  pages = {247--288},
  issn = {1042-9832, 1098-2418},
  doi = {10.1002/rsa.10004},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/rsa.10004},
  urldate = {2025-03-16},
  abstract = {Abstract                            We consider the problem of partitioning               n               randomly chosen integers between 1 and 2                                m                              into two subsets such that the discrepancy, the absolute value of the difference of their sums, is minimized. A partition is called               perfect               if the optimum discrepancy is 0 when the sum of all               n               integers in the original set is even, or 1 when the sum is odd. Parameterizing the random problem in terms of κ=               m               /               n               , we prove that the problem has a phase transition at κ=1, in the sense that for κ{$<$}1, there are many perfect partitions with probability tending to 1 as               n               →∞, whereas for κ{$>$}1, there are no perfect partitions with probability tending to 1. Moreover, we show that this transition is first‐order in the sense the derivative of the so‐called entropy is discontinuous at κ=1.                                         We also determine the finite‐size scaling window about the transition point: κ                                n                              =1−(2               n               )               −1               \,log               2               \,               n               +λ                                n                              /               n               , by showing that the probability of a perfect partition tends to 1,\,0, or some explicitly computable               p               (λ)∈(0,\,1), depending on whether λ                                n                              tends to −∞,\,∞, or λ∈(−∞,\,∞), respectively. For λ                                n                              →−∞ fast enough, we show that the number of perfect partitions is Gaussian in the limit. For λ                                n                              →∞, we prove that with high probability the optimum partition is unique, and that the optimum discrepancy is Θ(2                              ). Within the window, i.e., if |λ                                n                              | is bounded, we prove that the optimum discrepancy is bounded. Both for λ                                n                              →∞ and within the window, we find the limiting distribution of the (scaled) discrepancy. Finally, both for the integer partitioning problem and for the continuous partitioning problem, we find the joint distribution of the               k               smallest discrepancies above the scaling window.{$\quad$}© 2001 John Wiley \& Sons, Inc.{$\quad$}Random Struct. Alg., 19: 247–288, 2001},
  langid = {english}
}

@online{brennanOptimalAverageCaseReductions2019,
  title = {Optimal {{Average-Case Reductions}} to {{Sparse PCA}}: {{From Weak Assumptions}} to {{Strong Hardness}}},
  shorttitle = {Optimal {{Average-Case Reductions}} to {{Sparse PCA}}},
  author = {Brennan, Matthew and Bresler, Guy},
  date = {2019-02-20},
  eprint = {1902.07380},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.07380},
  url = {http://arxiv.org/abs/1902.07380},
  urldate = {2025-03-16},
  abstract = {In the past decade, sparse principal component analysis has emerged as an archetypal problem for illustrating statistical-computational tradeoffs. This trend has largely been driven by a line of research aiming to characterize the average-case complexity of sparse PCA through reductions from the planted clique (PC) conjecture - which conjectures that there is no polynomial-time algorithm to detect a planted clique of size \$K = o(N\textasciicircum\{1/2\})\$ in \$\textbackslash mathcal\{G\}(N, \textbackslash frac\{1\}\{2\})\$. All previous reductions to sparse PCA either fail to show tight computational lower bounds matching existing algorithms or show lower bounds for formulations of sparse PCA other than its canonical generative model, the spiked covariance model. Also, these lower bounds all quickly degrade with the exponent in the PC conjecture. Specifically, when only given the PC conjecture up to \$K = o(N\textasciicircum\textbackslash alpha)\$ where \$\textbackslash alpha {$<$} 1/2\$, there is no sparsity level \$k\$ at which these lower bounds remain tight. If \$\textbackslash alpha \textbackslash le 1/3\$ these reductions fail to even show the existence of a statistical-computational tradeoff at any sparsity \$k\$. We give a reduction from PC that yields the first full characterization of the computational barrier in the spiked covariance model, providing tight lower bounds at all sparsities \$k\$. We also show the surprising result that weaker forms of the PC conjecture up to clique size \$K = o(N\textasciicircum\textbackslash alpha)\$ for any given \$\textbackslash alpha \textbackslash in (0, 1/2]\$ imply tight computational lower bounds for sparse PCA at sparsities \$k = o(n\textasciicircum\{\textbackslash alpha/3\})\$. This shows that even a mild improvement in the signal strength needed by the best known polynomial-time sparse PCA algorithms would imply that the hardness threshold for PC is subpolynomial. This is the first instance of a suboptimal hardness assumption implying optimal lower bounds for another problem in unsupervised learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/ABD45JLJ/Brennan and Bresler - 2019 - Optimal Average-Case Reductions to Sparse PCA From Weak Assumptions to Strong Hardness.pdf;/Users/rushilma/Zotero/storage/N238QSVK/1902.html}
}

@online{brennanReducibilityComputationalLower2019,
  title = {Reducibility and {{Computational Lower Bounds}} for {{Problems}} with {{Planted Sparse Structure}}},
  author = {Brennan, Matthew and Bresler, Guy and Huleihel, Wasim},
  date = {2019-11-18},
  eprint = {1806.07508},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.07508},
  url = {http://arxiv.org/abs/1806.07508},
  urldate = {2025-03-16},
  abstract = {The prototypical high-dimensional statistics problem entails finding a structured signal in noise. Many of these problems exhibit an intriguing phenomenon: the amount of data needed by all known computationally efficient algorithms far exceeds what is needed for inefficient algorithms that search over all possible structures. A line of work initiated by Berthet and Rigollet in 2013 has aimed to explain these statistical-computational gaps by reducing from conjecturally hard average-case problems in computer science. However, the delicate nature of average-case reductions has limited the applicability of this approach. In this work we introduce several new techniques to give a web of average-case reductions showing strong computational lower bounds based on the planted clique conjecture using natural problems as intermediates. These include tight lower bounds for Planted Independent Set, Planted Dense Subgraph, Sparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block Model and a biased variant of Sparse PCA. We also give algorithms matching our lower bounds and identify the information-theoretic limits of the models we consider.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Information Theory,Mathematics - Information Theory,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/SF8XIF45/Brennan et al. - 2019 - Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure.pdf;/Users/rushilma/Zotero/storage/DL4QG8DK/1806.html}
}

@online{chandrasekaranIntegerFeasibilityRandom2013,
  title = {Integer {{Feasibility}} of {{Random Polytopes}}},
  author = {Chandrasekaran, Karthekeyan and Vempala, Santosh},
  date = {2013-08-23},
  eprint = {1111.4649},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1111.4649},
  url = {http://arxiv.org/abs/1111.4649},
  urldate = {2025-03-16},
  abstract = {We study integer programming instances over polytopes P(A,b)=\{x:Ax{$<$}=b\} where the constraint matrix A is random, i.e., its entries are i.i.d. Gaussian or, more generally, its rows are i.i.d. from a spherically symmetric distribution. The radius of the largest inscribed ball is closely related to the existence of integer points in the polytope. We show that for m=2\textasciicircum O(sqrt\{n\}), there exist constants c\_0 {$<$} c\_1 such that with high probability, random polytopes are integer feasible if the radius of the largest ball contained in the polytope is at least c\_1sqrt\{log(m/n)\}; and integer infeasible if the largest ball contained in the polytope is centered at (1/2,...,1/2) and has radius at most c\_0sqrt\{log(m/n)\}. Thus, random polytopes transition from having no integer points to being integer feasible within a constant factor increase in the radius of the largest inscribed ball. We show integer feasibility via a randomized polynomial-time algorithm for finding an integer point in the polytope. Our main tool is a simple new connection between integer feasibility and linear discrepancy. We extend a recent algorithm for finding low-discrepancy solutions (Lovett-Meka, FOCS '12) to give a constructive upper bound on the linear discrepancy of random matrices. By our connection between discrepancy and integer feasibility, this upper bound on linear discrepancy translates to the radius lower bound that guarantees integer feasibility of random polytopes.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/AGPAATWL/Chandrasekaran and Vempala - 2013 - Integer Feasibility of Random Polytopes.pdf;/Users/rushilma/Zotero/storage/SABH3HNE/1111.html}
}

@article{chenSuboptimalityLocalAlgorithms2019,
  title = {Suboptimality of Local Algorithms for a Class of Max-Cut Problems},
  author = {Chen, Wei-Kuo and Gamarnik, David and Panchenko, Dmitry and Rahman, Mustazee},
  date = {2019-05-01},
  journaltitle = {The Annals of Probability},
  shortjournal = {Ann. Probab.},
  volume = {47},
  number = {3},
  eprint = {1707.05386},
  eprinttype = {arXiv},
  eprintclass = {math},
  issn = {0091-1798},
  doi = {10.1214/18-AOP1291},
  url = {http://arxiv.org/abs/1707.05386},
  urldate = {2025-03-16},
  abstract = {We show that in random \$K\$-uniform hypergraphs of constant average degree, for even \$K \textbackslash geq 4\$, local algorithms defined as factors of i.i.d. can not find nearly maximal cuts, when the average degree is sufficiently large. These algorithms have been used frequently to obtain lower bounds for the max-cut problem on random graphs, but it was not known whether they could be successful in finding nearly maximal cuts. This result follows from the fact that the overlap of any two nearly maximal cuts in such hypergraphs does not take values in a certain non-trivial interval - a phenomenon referred to as the overlap gap property - which is proved by comparing diluted models with large average degree with appropriate fully connected spin glass models and showing the overlap gap property in the latter setting.},
  keywords = {Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/TLALKUKY/Chen et al. - 2019 - Suboptimality of local algorithms for a class of max-cut problems.pdf;/Users/rushilma/Zotero/storage/KSGUK2DF/1707.html}
}

@article{coffmanjr.ApplicationBinPackingMultiprocessor1978,
  title = {An {{Application}} of {{Bin-Packing}} to {{Multiprocessor Scheduling}}},
  author = {Coffman, Jr., E. G. and Garey, M. R. and Johnson, D. S.},
  date = {1978-02},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  volume = {7},
  number = {1},
  pages = {1--17},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/0207001},
  url = {https://epubs.siam.org/doi/abs/10.1137/0207001},
  urldate = {2025-03-15},
  abstract = {Motivated by potential applications to computer storage allocation, we generalize the classical one-dimensional bin packing model to include dynamic arrivals and departures of items over time. Within this setting, we prove close upper and lower bounds on the worst-case performance of the commonly used First Fit packing algorithm, and, using adversary-type arguments, we show that no on-line packing algorithm can satisfy a substantially better performance bound than that for First Fit.}
}

@book{coffmanProbabilisticAnalysisPacking1991,
  title = {Probabilistic Analysis of Packing and Partitioning Algorithms},
  author = {Coffman, Edward Grady and Lueker, George S.},
  date = {1991},
  series = {Wiley-Interscience Series in Discrete Mathematics and Optimization},
  publisher = {J. Wiley \& sons},
  location = {New York},
  isbn = {978-0-471-53272-9},
  langid = {english}
}

@article{coja-oghlanIndependentSetsRandom2015,
  title = {On Independent Sets in Random Graphs},
  author = {Coja-Oghlan, Amin and Efthymiou, Charilaos},
  date = {2015-10},
  journaltitle = {Random Structures \& Algorithms},
  shortjournal = {Random Struct. Alg.},
  volume = {47},
  number = {3},
  eprint = {1007.1378},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {436--486},
  issn = {10429832},
  doi = {10.1002/rsa.20550},
  url = {http://arxiv.org/abs/1007.1378},
  urldate = {2025-03-16},
  abstract = {The independence number of a sparse random graph G(n,m) of average degree d=2m/n is well-known to be \textbackslash alpha(G(n,m))\textasciitilde 2n ln(d)/d with high probability. Moreover, a trivial greedy algorithm w.h.p. finds an independent set of size (1+o(1)) n ln(d)/d, i.e. half the maximum size. Yet in spite of 30 years of extensive research no efficient algorithm has emerged to produce an independent set with (1+c)n ln(d)/d, for any fixed c{$>$}0. In this paper we prove that the combinatorial structure of the independent set problem in random graphs undergoes a phase transition as the size k of the independent sets passes the point k nln(d)/d. Roughly speaking, we prove that independent sets of size k{$>$}(1+c)n ln(d)/d form an intricately ragged landscape, in which local search algorithms are bound to get stuck. We illustrate this phenomenon by providing an exponential lower bound for the Metropolis process, a Markov chain for sampling independents sets.},
  keywords = {Computer Science - Discrete Mathematics},
  file = {/Users/rushilma/Zotero/storage/J3FRAIVD/Coja-Oghlan and Efthymiou - 2015 - On independent sets in random graphs.pdf;/Users/rushilma/Zotero/storage/INVFQRLR/1007.html}
}

@article{corusArtificialImmuneSystems2019,
  title = {Artificial Immune Systems Can Find Arbitrarily Good Approximations for the {{NP-hard}} Number Partitioning Problem},
  author = {Corus, Dogan and Oliveto, Pietro S. and Yazdani, Donya},
  date = {2019-09},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {274},
  pages = {180--196},
  issn = {00043702},
  doi = {10.1016/j.artint.2019.03.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000437021830345X},
  urldate = {2025-03-15},
  langid = {english},
  file = {/Users/rushilma/Zotero/storage/Q2Q2YC22/Corus et al. - 2019 - Artificial immune systems can find arbitrarily good approximations for the NP-hard number partitioni.pdf}
}

@online{deshpandeImprovedSumofSquaresLower2015,
  title = {Improved {{Sum-of-Squares Lower Bounds}} for {{Hidden Clique}} and {{Hidden Submatrix Problems}}},
  author = {Deshpande, Yash and Montanari, Andrea},
  date = {2015-02-23},
  eprint = {1502.06590},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1502.06590},
  url = {http://arxiv.org/abs/1502.06590},
  urldate = {2025-03-16},
  abstract = {Given a large data matrix \$A\textbackslash in\textbackslash mathbb\{R\}\textasciicircum\{n\textbackslash times n\}\$, we consider the problem of determining whether its entries are i.i.d. with some known marginal distribution \$A\_\{ij\}\textbackslash sim P\_0\$, or instead \$A\$ contains a principal submatrix \$A\_\{\{\textbackslash sf Q\},\{\textbackslash sf Q\}\}\$ whose entries have marginal distribution \$A\_\{ij\}\textbackslash sim P\_1\textbackslash neq P\_0\$. As a special case, the hidden (or planted) clique problem requires to find a planted clique in an otherwise uniformly random graph. Assuming unbounded computational resources, this hypothesis testing problem is statistically solvable provided \$|\{\textbackslash sf Q\}|\textbackslash ge C \textbackslash log n\$ for a suitable constant \$C\$. However, despite substantial effort, no polynomial time algorithm is known that succeeds with high probability when \$|\{\textbackslash sf Q\}| = o(\textbackslash sqrt\{n\})\$. Recently Meka and Wigderson \textbackslash cite\{meka2013association\}, proposed a method to establish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy. Here we consider the degree-\$4\$ SOS relaxation, and study the construction of \textbackslash cite\{meka2013association\} to prove that SOS fails unless \$k\textbackslash ge C\textbackslash, n\textasciicircum\{1/3\}/\textbackslash log n\$. An argument presented by Barak implies that this lower bound cannot be substantially improved unless the witness construction is changed in the proof. Our proof uses the moments method to bound the spectrum of a certain random association scheme, i.e. a symmetric random matrix whose rows and columns are indexed by the edges of an Erd\textbackslash "os-Renyi random graph.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Information Theory,Mathematics - Information Theory,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/5SAFFRWF/Deshpande and Montanari - 2015 - Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems.pdf;/Users/rushilma/Zotero/storage/M27BGTE9/1502.html}
}

@online{diakonikolasStatisticalQueryLower2017,
  title = {Statistical {{Query Lower Bounds}} for {{Robust Estimation}} of {{High-dimensional Gaussians}} and {{Gaussian Mixtures}}},
  author = {Diakonikolas, Ilias and Kane, Daniel M. and Stewart, Alistair},
  date = {2017-05-17},
  eprint = {1611.03473},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1611.03473},
  url = {http://arxiv.org/abs/1611.03473},
  urldate = {2025-03-16},
  abstract = {We describe a general technique that yields the first \{\textbackslash em Statistical Query lower bounds\} for a range of fundamental high-dimensional learning problems involving Gaussian distributions. Our main results are for the problems of (1) learning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of a single unknown Gaussian distribution. For each of these problems, we show a \{\textbackslash em super-polynomial gap\} between the (information-theoretic) sample complexity and the computational complexity of \{\textbackslash em any\} Statistical Query algorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively matched by known learning algorithms for GMMs. Our lower bound for Problem (2) implies that the accuracy of the robust learning algorithm in\textasciitilde\textbackslash cite\{DiakonikolasKKLMS16\} is essentially best possible among all polynomial-time SQ algorithms. Our SQ lower bounds are attained via a unified moment-matching technique that is useful in other contexts and may be of broader interest. Our technique yields nearly-tight lower bounds for a number of related unsupervised estimation problems. Specifically, for the problems of (3) robust covariance estimation in spectral norm, and (4) robust sparse mean estimation, we establish a quadratic \{\textbackslash em statistical--computational tradeoff\} for SQ algorithms, matching known upper bounds. Finally, our technique can be used to obtain tight sample complexity lower bounds for high-dimensional \{\textbackslash em testing\} problems. Specifically, for the classical problem of robustly \{\textbackslash em testing\} an unknown mean (known covariance) Gaussian, our technique implies an information-theoretic sample lower bound that scales \{\textbackslash em linearly\} in the dimension. Our sample lower bound matches the sample complexity of the corresponding robust \{\textbackslash em learning\} problem and separates the sample complexity of robust testing from standard (non-robust) testing.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/2S47BH8U/Diakonikolas et al. - 2017 - Statistical Query Lower Bounds for Robust Estimation of High-dimensional Gaussians and Gaussian Mixt.pdf;/Users/rushilma/Zotero/storage/JELQ7J4P/1611.html}
}

@online{feldmanStatisticalAlgorithmsLower2016,
  title = {Statistical {{Algorithms}} and a {{Lower Bound}} for {{Detecting Planted Clique}}},
  author = {Feldman, Vitaly and Grigorescu, Elena and Reyzin, Lev and Vempala, Santosh and Xiao, Ying},
  date = {2016-08-15},
  eprint = {1201.1214},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1201.1214},
  url = {http://arxiv.org/abs/1201.1214},
  urldate = {2025-03-16},
  abstract = {We introduce a framework for proving lower bounds on computational problems over distributions against algorithms that can be implemented using access to a statistical query oracle. For such algorithms, access to the input distribution is limited to obtaining an estimate of the expectation of any given function on a sample drawn randomly from the input distribution, rather than directly accessing samples. Most natural algorithms of interest in theory and in practice, e.g., moments-based methods, local search, standard iterative methods for convex optimization, MCMC and simulated annealing can be implemented in this framework. Our framework is based on, and generalizes, the statistical query model in learning theory (Kearns, 1998). Our main application is a nearly optimal lower bound on the complexity of any statistical query algorithm for detecting planted bipartite clique distributions (or planted dense subgraph distributions) when the planted clique has size \$O(n\textasciicircum\{1/2-\textbackslash delta\})\$ for any constant \$\textbackslash delta {$>$} 0\$. The assumed hardness of variants of these problems has been used to prove hardness of several other problems and as a guarantee for security in cryptographic applications. Our lower bounds provide concrete evidence of hardness, thus supporting these assumptions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms},
  file = {/Users/rushilma/Zotero/storage/CQ7NASE3/Feldman et al. - 2016 - Statistical Algorithms and a Lower Bound for Detecting Planted Clique.pdf;/Users/rushilma/Zotero/storage/UX2C3DDD/1201.html}
}

@article{ferreiraProbabilisticAnalysisNumber1998,
  title = {Probabilistic Analysis of the Number Partitioning Problem},
  author = {Ferreira, F. F. and Fontanari, J. F.},
  date = {1998-04},
  journaltitle = {Journal of Physics A: Mathematical and General},
  shortjournal = {J. Phys. A: Math. Gen.},
  volume = {31},
  number = {15},
  pages = {3417},
  issn = {0305-4470},
  doi = {10.1088/0305-4470/31/15/007},
  url = {https://dx.doi.org/10.1088/0305-4470/31/15/007},
  urldate = {2025-03-15},
  abstract = {Given a sequence of N positive real numbers , the number partitioning problem consists of partitioning them into two sets such that the absolute value of the difference of the sums of over the two sets is minimized. In the case in which the 's are statistically independent random variables uniformly distributed in the unit interval, this NP-complete problem is equivalent to the problem of finding the ground state of an infinite-range, random antiferromagnetic Ising model. We employ the annealed approximation to derive analytical lower bounds to the average value of the difference for the best constrained and unconstrained partitions in the large N limit. Furthermore, we calculate analytically the fraction of metastable states, i.e. states that are stable against all single spin flips, and found that it vanishes like .},
  langid = {english}
}

@online{gamarnikAlgorithmicObstructionsRandom2021b,
  title = {Algorithmic {{Obstructions}} in the {{Random Number Partitioning Problem}}},
  author = {Gamarnik, David and Kızıldağ, Eren C.},
  date = {2021-03-02},
  eprint = {2103.01369},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2103.01369},
  url = {http://arxiv.org/abs/2103.01369},
  urldate = {2025-03-15},
  abstract = {We consider the algorithmic problem of finding a near-optimal solution for the number partitioning problem (NPP). The NPP appears in many applications, including the design of randomized controlled trials, multiprocessor scheduling, and cryptography; and is also of theoretical significance. It possesses a so-called statistical-to-computational gap: when its input \$X\$ has distribution \$\textbackslash mathcal\{N\}(0,I\_n)\$, its optimal value is \$\textbackslash Theta(\textbackslash sqrt\{n\}2\textasciicircum\{-n\})\$ w.h.p.; whereas the best polynomial-time algorithm achieves an objective value of only \$2\textasciicircum\{-\textbackslash Theta(\textbackslash log\textasciicircum 2 n)\}\$, w.h.p. In this paper, we initiate the study of the nature of this gap. Inspired by insights from statistical physics, we study the landscape of NPP and establish the presence of the Overlap Gap Property (OGP), an intricate geometric property which is known to be a rigorous evidence of an algorithmic hardness for large classes of algorithms. By leveraging the OGP, we establish that (a) any sufficiently stable algorithm, appropriately defined, fails to find a near-optimal solution with energy below \$2\textasciicircum\{-\textbackslash omega(n \textbackslash log\textasciicircum\{-1/5\} n)\}\$; and (b) a very natural MCMC dynamics fails to find near-optimal solutions. Our simulations suggest that the state of the art algorithm achieving \$2\textasciicircum\{-\textbackslash Theta(\textbackslash log\textasciicircum 2 n)\}\$ is indeed stable, but formally verifying this is left as an open problem. OGP regards the overlap structure of \$m-\$tuples of solutions achieving a certain objective value. When \$m\$ is constant we prove the presence of OGP in the regime \$2\textasciicircum\{-\textbackslash Theta(n)\}\$, and the absence of it in the regime \$2\textasciicircum\{-o(n)\}\$. Interestingly, though, by considering overlaps with growing values of \$m\$ we prove the presence of the OGP up to the level \$2\textasciicircum\{-\textbackslash omega(\textbackslash sqrt\{n\textbackslash log n\})\}\$. Our proof of the failure of stable algorithms at values \$2\textasciicircum\{-\textbackslash omega(n \textbackslash log\textasciicircum\{-1/5\} n)\}\$ employs methods from Ramsey Theory from the extremal combinatorics, and is of independent interest.},
  pubstate = {prepublished},
  keywords = {Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/VHM5KWJ7/Gamarnik and Kızıldağ - 2021 - Algorithmic Obstructions in the Random Number Partitioning Problem.pdf;/Users/rushilma/Zotero/storage/P669R67F/2103.html}
}

@online{gamarnikAlgorithmsBarriersSymmetric2022,
  title = {Algorithms and {{Barriers}} in the {{Symmetric Binary Perceptron Model}}},
  author = {Gamarnik, David and Kızıldağ, Eren C. and Perkins, Will and Xu, Changji},
  date = {2022-03-29},
  eprint = {2203.15667},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.15667},
  url = {http://arxiv.org/abs/2203.15667},
  urldate = {2025-03-15},
  abstract = {The symmetric binary perceptron (\$\textbackslash texttt\{SBP\}\$) exhibits a dramatic statistical-to-computational gap: the densities at which known efficient algorithms find solutions are far below the threshold for the existence of solutions. Furthermore, the \$\textbackslash texttt\{SBP\}\$ exhibits a striking structural property: at all positive constraint densities almost all of its solutions are 'totally frozen' singletons separated by large Hamming distance \textbackslash cite\{perkins2021frozen,abbe2021proof\}. This suggests that finding a solution to the \$\textbackslash texttt\{SBP\}\$ may be computationally intractable. At the same time, the \$\textbackslash texttt\{SBP\}\$ does admit polynomial-time search algorithms at low enough densities. A conjectural explanation for this conundrum was put forth in \textbackslash cite\{baldassi2020clustering\}: efficient algorithms succeed in the face of freezing by finding exponentially rare clusters of large size. However, it was discovered recently that such rare large clusters exist at all subcritical densities, even at those well above the limits of known efficient algorithms \textbackslash cite\{abbe2021binary\}. Thus the driver of the statistical-to-computational gap exhibited by this model remains a mystery. In this paper, we conduct a different landscape analysis to explain the algorithmic tractability of this problem. We show that at high enough densities the \$\textbackslash texttt\{SBP\}\$ exhibits the multi Overlap Gap Property (\$m-\$OGP), an intricate geometrical property known to be a rigorous barrier for large classes of algorithms. Our analysis shows that the \$m-\$OGP threshold (a) is well below the satisfiability threshold; and (b) matches the best known algorithmic threshold up to logarithmic factors as \$m\textbackslash to\textbackslash infty\$. We then prove that the \$m-\$OGP rules out the class of stable algorithms for the \$\textbackslash texttt\{SBP\}\$ above this threshold. We conjecture that the \$m \textbackslash to \textbackslash infty\$ limit of the \$m\$-OGP threshold marks the algorithmic threshold for the problem.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/XPJC8GGJ/Gamarnik et al. - 2022 - Algorithms and Barriers in the Symmetric Binary Perceptron Model.pdf;/Users/rushilma/Zotero/storage/ES5XP8DG/2203.html}
}

@article{gamarnikComputingPartitionFunction2021,
  title = {Computing the Partition Function of the {{Sherrington-Kirkpatrick}} Model Is Hard on Average},
  author = {Gamarnik, David and Kizildag, Eren},
  date = {2021-06-01},
  journaltitle = {The Annals of Applied Probability},
  shortjournal = {Ann. Appl. Probab.},
  volume = {31},
  number = {3},
  eprint = {1810.05907},
  eprinttype = {arXiv},
  eprintclass = {math},
  issn = {1050-5164},
  doi = {10.1214/20-AAP1625},
  url = {http://arxiv.org/abs/1810.05907},
  urldate = {2025-03-16},
  abstract = {We establish the average-case hardness of the algorithmic problem of exact computation of the partition function associated with the Sherrington-Kirkpatrick model of spin glasses with Gaussian couplings and random external field. In particular, we establish that unless \$P= \textbackslash\#P\$, there does not exist a polynomial-time algorithm to exactly compute the partition function on average. This is done by showing that if there exists a polynomial time algorithm, which exactly computes the partition function for inverse polynomial fraction (\$1/n\textasciicircum\{O(1)\}\$) of all inputs, then there is a polynomial time algorithm, which exactly computes the partition function for all inputs, with high probability, yielding \$P=\textbackslash\#P\$. The computational model that we adopt is \{\textbackslash em finite-precision arithmetic\}, where the algorithmic inputs are truncated first to a certain level \$N\$ of digital precision. The ingredients of our proof include the random and downward self-reducibility of the partition function with random external field; an argument of Cai et al. \textbackslash cite\{cai1999hardness\} for establishing the average-case hardness of computing the permanent of a matrix; a list-decoding algorithm of Sudan \textbackslash cite\{sudan1996maximum\}, for reconstructing polynomials intersecting a given list of numbers at sufficiently many points; and near-uniformity of the log-normal distribution, modulo a large prime \$p\$. To the best of our knowledge, our result is the first one establishing a provable hardness of a model arising in the field of spin glasses. Furthermore, we extend our result to the same problem under a different \{\textbackslash em real-valued\} computational model, e.g. using a Blum-Shub-Smale machine \textbackslash cite\{blum1988theory\} operating over real-valued inputs.},
  keywords = {Computer Science - Computational Complexity,Condensed Matter - Statistical Mechanics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/J6CKYDNI/Gamarnik and Kizildag - 2021 - Computing the partition function of the Sherrington-Kirkpatrick model is hard on average.pdf;/Users/rushilma/Zotero/storage/88ZR2ETA/1810.html}
}

@online{gamarnikFindingLargeSubmatrix2016,
  title = {Finding a {{Large Submatrix}} of a {{Gaussian Random Matrix}}},
  author = {Gamarnik, David and Li, Quan},
  date = {2016-02-26},
  eprint = {1602.08529},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1602.08529},
  url = {http://arxiv.org/abs/1602.08529},
  urldate = {2025-03-29},
  abstract = {We consider the problem of finding a \$k\textbackslash times k\$ submatrix of an \$n\textbackslash times n\$ matrix with i.i.d. standard Gaussian entries, which has a large average entry. It was shown earlier by Bhamidi et al. that the largest average value of such a matrix is \$2\textbackslash sqrt\{\textbackslash log n/k\}\$ with high probability. In the same paper an evidence was provided that a natural greedy algorithm called Largest Average Submatrix (\$\textbackslash LAS\$) should produce a matrix with average entry approximately \$\textbackslash sqrt\{2\}\$ smaller. In this paper we show that the matrix produced by the \$\textbackslash LAS\$ algorithm is indeed \$\textbackslash sqrt\{2\textbackslash log n/k\}\$ w.h.p. Then by drawing an analogy with the problem of finding cliques in random graphs, we propose a simple greedy algorithm which produces a \$k\textbackslash times k\$ matrix with asymptotically the same average value. Since the greedy algorithm is the best known algorithm for finding cliques in random graphs, it is tempting to believe that beating the factor \$\textbackslash sqrt\{2\}\$ performance gap suffered by both algorithms might be very challenging. Surprisingly, we show the existence of a very simple algorithm which produces a matrix with average value \$(4/3)\textbackslash sqrt\{2\textbackslash log n/k\}\$. To get an insight into the algorithmic hardness of this problem, and motivated by methods originating in the theory of spin glasses, we conduct the so-called expected overlap analysis of matrices with average value asymptotically \$\textbackslash alpha\textbackslash sqrt\{2\textbackslash log n/k\}\$. The overlap corresponds to the number of common rows and common columns for pairs of matrices achieving this value. We discover numerically an intriguing phase transition at \$\textbackslash alpha\textasciicircum *\textbackslash approx 1.3608..\$: when \$\textbackslash alpha{$<\backslash$}alpha\textasciicircum *\$ the space of overlaps is a continuous subset of \$[0,1]\textasciicircum 2\$, whereas \$\textbackslash alpha=\textbackslash alpha\textasciicircum *\$ marks the onset of discontinuity, and the model exhibits the Overlap Gap Property when \$\textbackslash alpha{$>\backslash$}alpha\textasciicircum *\$. We conjecture that \$\textbackslash alpha{$>\backslash$}alpha\textasciicircum *\$ marks the onset of the algorithmic hardness.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematics - Probability,Mathematics - Statistics Theory,Physics - Data Analysis Statistics and Probability,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/JL3SSB6C/Gamarnik and Li - 2016 - Finding a Large Submatrix of a Gaussian Random Matrix.pdf;/Users/rushilma/Zotero/storage/BHK4TX3L/1602.html}
}

@online{gamarnikHardnessRandomOptimization2022a,
  title = {Hardness of {{Random Optimization Problems}} for {{Boolean Circuits}}, {{Low-Degree Polynomials}}, and {{Langevin Dynamics}}},
  author = {Gamarnik, David and Jagannath, Aukosh and Wein, Alexander S.},
  date = {2022-01-26},
  eprint = {2004.12063},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.12063},
  url = {http://arxiv.org/abs/2004.12063},
  urldate = {2025-03-15},
  abstract = {We consider the problem of finding nearly optimal solutions of optimization problems with random objective functions. Two concrete problems we consider are (a) optimizing the Hamiltonian of a spherical or Ising \$p\$-spin glass model, and (b) finding a large independent set in a sparse Erd\textbackslash H\{o\}s-R\textbackslash '\{e\}nyi graph. The following families of algorithms are considered: (a) low-degree polynomials of the input; (b) low-depth Boolean circuits; (c) the Langevin dynamics algorithm. We show that these families of algorithms fail to produce nearly optimal solutions with high probability. For the case of Boolean circuits, our results improve the state-of-the-art bounds known in circuit complexity theory (although we consider the search problem as opposed to the decision problem). Our proof uses the fact that these models are known to exhibit a variant of the overlap gap property (OGP) of near-optimal solutions. Specifically, for both models, every two solutions whose objectives are above a certain threshold are either close or far from each other. The crux of our proof is that the classes of algorithms we consider exhibit a form of stability. We show by an interpolation argument that stable algorithms cannot overcome the OGP barrier. The stability of Langevin dynamics is an immediate consequence of the well-posedness of stochastic differential equations. The stability of low-degree polynomials and Boolean circuits is established using tools from Gaussian and Boolean analysis -- namely hypercontractivity and total influence, as well as a novel lower bound for random walks avoiding certain subsets. In the case of Boolean circuits, the result also makes use of Linal-Mansour-Nisan's classical theorem. Our techniques apply more broadly to low influence functions and may apply more generally.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/QN2AA547/Gamarnik et al. - 2022 - Hardness of Random Optimization Problems for Boolean Circuits, Low-Degree Polynomials, and Langevin.pdf;/Users/rushilma/Zotero/storage/YBV35CJV/2004.html}
}

@online{gamarnikHighDimensionalRegressionBinary2019,
  title = {High-{{Dimensional Regression}} with {{Binary Coefficients}}. {{Estimating Squared Error}} and a {{Phase Transition}}},
  author = {Gamarnik, David and Zadik, Ilias},
  date = {2019-09-25},
  eprint = {1701.04455},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1701.04455},
  url = {http://arxiv.org/abs/1701.04455},
  urldate = {2025-03-16},
  abstract = {We consider a sparse linear regression model Y=X\textbackslash beta\textasciicircum\{*\}+W where X has a Gaussian entries, W is the noise vector with mean zero Gaussian entries, and \textbackslash beta\textasciicircum\{*\} is a binary vector with support size (sparsity) k. Using a novel conditional second moment method we obtain a tight up to a multiplicative constant approximation of the optimal squared error \textbackslash min\_\{\textbackslash beta\}\textbackslash |Y-X\textbackslash beta\textbackslash |\_\{2\}, where the minimization is over all k-sparse binary vectors \textbackslash beta. The approximation reveals interesting structural properties of the underlying regression problem. In particular, a) We establish that n\textasciicircum *=2k\textbackslash log p/\textbackslash log (2k/\textbackslash sigma\textasciicircum\{2\}+1) is a phase transition point with the following "all-or-nothing" property. When n exceeds n\textasciicircum\{*\}, (2k)\textasciicircum\{-1\}\textbackslash |\textbackslash beta\_\{2\}-\textbackslash beta\textasciicircum *\textbackslash |\_0\textbackslash approx 0, and when n is below n\textasciicircum\{*\}, (2k)\textasciicircum\{-1\}\textbackslash |\textbackslash beta\_\{2\}-\textbackslash beta\textasciicircum *\textbackslash |\_0\textbackslash approx 1, where \textbackslash beta\_2 is the optimal solution achieving the smallest squared error. With this we prove that n\textasciicircum\{*\} is the asymptotic threshold for recovering \textbackslash beta\textasciicircum * information theoretically. b) We compute the squared error for an intermediate problem \textbackslash min\_\{\textbackslash beta\}\textbackslash |Y-X\textbackslash beta\textbackslash |\_\{2\} where minimization is restricted to vectors \textbackslash beta with \textbackslash |\textbackslash beta-\textbackslash beta\textasciicircum\{*\}\textbackslash |\_0=2k \textbackslash zeta, for \textbackslash zeta\textbackslash in [0,1]. We show that a lower bound part \textbackslash Gamma(\textbackslash zeta) of the estimate, which corresponds to the estimate based on the first moment method, undergoes a phase transition at three different thresholds, namely n\_\{\textbackslash text\{inf,1\}\}=\textbackslash sigma\textasciicircum 2\textbackslash log p, which is information theoretic bound for recovering \textbackslash beta\textasciicircum * when k=1 and \textbackslash sigma is large, then at n\textasciicircum\{*\} and finally at n\_\{\textbackslash text\{LASSO/CS\}\}. c) We establish a certain Overlap Gap Property (OGP) on the space of all binary vectors \textbackslash beta when n\textbackslash le ck\textbackslash log p for sufficiently small constant c. We conjecture that OGP is the source of algorithmic hardness of solving the minimization problem \textbackslash min\_\{\textbackslash beta\}\textbackslash |Y-X\textbackslash beta\textbackslash |\_\{2\} in the regime n},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/VLKXFRK2/Gamarnik and Zadik - 2019 - High-Dimensional Regression with Binary Coefficients. Estimating Squared Error and a Phase Transitio.pdf;/Users/rushilma/Zotero/storage/LKZ7QW6I/1701.html}
}

@online{gamarnikLandscapePlantedClique2019,
  title = {The {{Landscape}} of the {{Planted Clique Problem}}: {{Dense}} Subgraphs and the {{Overlap Gap Property}}},
  shorttitle = {The {{Landscape}} of the {{Planted Clique Problem}}},
  author = {Gamarnik, David and Zadik, Ilias},
  date = {2019-12-30},
  eprint = {1904.07174},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1904.07174},
  url = {http://arxiv.org/abs/1904.07174},
  urldate = {2025-03-16},
  abstract = {In this paper we study the computational-statistical gap of the planted clique problem, where a clique of size \$k\$ is planted in an Erdos Renyi graph \$G(n,\textbackslash frac\{1\}\{2\})\$ resulting in a graph \$G\textbackslash left(n,\textbackslash frac\{1\}\{2\},k\textbackslash right)\$. The goal is to recover the planted clique vertices by observing \$G\textbackslash left(n,\textbackslash frac\{1\}\{2\},k\textbackslash right)\$ . It is known that the clique can be recovered as long as \$k \textbackslash geq \textbackslash left(2+\textbackslash epsilon\textbackslash right)\textbackslash log n \$ for any \$\textbackslash epsilon{$>$}0\$, but no polynomial-time algorithm is known for this task unless \$k=\textbackslash Omega\textbackslash left(\textbackslash sqrt\{n\} \textbackslash right)\$. Following a statistical-physics inspired point of view as an attempt to understand this computational-statistical gap, we study the landscape of the "sufficiently dense" subgraphs of \$G\$ and their overlap with the planted clique. Using the first moment method, we study the densest subgraph problems for subgraphs with fixed, but arbitrary, overlap size with the planted clique, and provide evidence of a phase transition for the presence of Overlap Gap Property (OGP) at \$k=\textbackslash Theta\textbackslash left(\textbackslash sqrt\{n\}\textbackslash right)\$. OGP is a concept introduced originally in spin glass theory and known to suggest algorithmic hardness when it appears. We establish the presence of OGP when \$k\$ is a small positive power of \$n\$ by using a conditional second moment method. As our main technical tool, we establish the first, to the best of our knowledge, concentration results for the \$K\$-densest subgraph problem for the Erdos-Renyi model \$G\textbackslash left(n,\textbackslash frac\{1\}\{2\}\textbackslash right)\$ when \$K=n\textasciicircum\{0.5-\textbackslash epsilon\}\$ for arbitrary \$\textbackslash epsilon{$>$}0\$. Finally, to study the OGP we employ a certain form of overparametrization, which is conceptually aligned with a large body of recent work in learning theory and optimization.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/NYQH8XN9/Gamarnik and Zadik - 2019 - The Landscape of the Planted Clique Problem Dense subgraphs and the Overlap Gap Property.pdf;/Users/rushilma/Zotero/storage/XMQKPKGA/1904.html}
}

@online{gamarnikLimitsLocalAlgorithms2013,
  title = {Limits of Local Algorithms over Sparse Random Graphs},
  author = {Gamarnik, David and Sudan, Madhu},
  date = {2013-04-05},
  eprint = {1304.1831},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1304.1831},
  url = {http://arxiv.org/abs/1304.1831},
  urldate = {2025-03-15},
  abstract = {Local algorithms on graphs are algorithms that run in parallel on the nodes of a graph to compute some global structural feature of the graph. Such algorithms use only local information available at nodes to determine local aspects of the global structure, while also potentially using some randomness. Recent research has shown that such algorithms show significant promise in computing structures like large independent sets in graphs locally. Indeed the promise led to a conjecture by Hatami, \textbackslash Lovasz and Szegedy \textbackslash cite\{HatamiLovaszSzegedy\} that local algorithms may be able to compute maximum independent sets in (sparse) random \$d\$-regular graphs. In this paper we refute this conjecture and show that every independent set produced by local algorithms is multiplicative factor \$1/2+1/(2\textbackslash sqrt\{2\})\$ smaller than the largest, asymptotically as \$d\textbackslash rightarrow\textbackslash infty\$. Our result is based on an important clustering phenomena predicted first in the literature on spin glasses, and recently proved rigorously for a variety of constraint satisfaction problems on random graphs. Such properties suggest that the geometry of the solution space can be quite intricate. The specific clustering property, that we prove and apply in this paper shows that typically every two large independent sets in a random graph either have a significant intersection, or have a nearly empty intersection. As a result, large independent sets are clustered according to the proximity to each other. While the clustering property was postulated earlier as an obstruction for the success of local algorithms, such as for example, the Belief Propagation algorithm, our result is the first one where the clustering property is used to formally prove limits on local algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Distributed Parallel and Cluster Computing,Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/EFZRIQIF/Gamarnik and Sudan - 2013 - Limits of local algorithms over sparse random graphs.pdf;/Users/rushilma/Zotero/storage/UV8QU69L/1304.html}
}

@online{gamarnikOverlapGapProperty2019,
  title = {The {{Overlap Gap Property}} and {{Approximate Message Passing Algorithms}} for \$p\$-Spin Models},
  author = {Gamarnik, David and Jagannath, Aukosh},
  date = {2019-11-26},
  eprint = {1911.06943},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1911.06943},
  url = {http://arxiv.org/abs/1911.06943},
  urldate = {2025-03-16},
  abstract = {We consider the algorithmic problem of finding a near ground state (near optimal solution) of a \$p\$-spin model. We show that for a class of algorithms broadly defined as Approximate Message Passing (AMP), the presence of the Overlap Gap Property (OGP), appropriately defined, is a barrier. We conjecture that when \$p\textbackslash ge 4\$ the model does indeed exhibits OGP (and prove it for the space of binary solutions). Assuming the validity of this conjecture, as an implication, the AMP fails to find near ground states in these models, per our result. We extend our result to the problem of finding pure states by means of Thouless, Anderson and Palmer (TAP) based iterations, which is yet another example of AMP type algorithms. We show that such iterations fail to find pure states approximately, subject to the conjecture that the space of pure states exhibits the OGP, appropriately stated, when \$p\textbackslash ge 4\$.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/E6RAPH4D/Gamarnik and Jagannath - 2019 - The Overlap Gap Property and Approximate Message Passing Algorithms for $p$-spin models.pdf;/Users/rushilma/Zotero/storage/UY7A4EDW/1911.html}
}

@article{gamarnikOverlapGapProperty2021,
  title = {The {{Overlap Gap Property}}: A {{Geometric Barrier}} to {{Optimizing}} over {{Random Structures}}},
  shorttitle = {The {{Overlap Gap Property}}},
  author = {Gamarnik, David},
  date = {2021-10-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {118},
  number = {41},
  eprint = {2109.14409},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {e2108492118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2108492118},
  url = {http://arxiv.org/abs/2109.14409},
  urldate = {2025-01-17},
  abstract = {The problem of optimizing over random structures emerges in many areas of science and engineering, ranging from statistical physics to machine learning and artificial intelligence. For many such structures finding optimal solutions by means of fast algorithms is not known and often is believed not possible. At the same time the formal hardness of these problems in form of say complexity-theoretic N P -hardness is lacking.},
  langid = {english},
  keywords = {Computer Science - Computational Complexity,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/EFY7NBSI/Gamarnik - 2021 - The Overlap Gap Property a Geometric Barrier to Optimizing over Random Structures.pdf}
}

@article{gamarnikOverlapGapProperty2021a,
  title = {The {{Overlap Gap Property}} in {{Principal Submatrix Recovery}}},
  author = {Gamarnik, David and Jagannath, Aukosh and Sen, Subhabrata},
  date = {2021-12},
  journaltitle = {Probability Theory and Related Fields},
  shortjournal = {Probab. Theory Relat. Fields},
  volume = {181},
  number = {4},
  eprint = {1908.09959},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {757--814},
  issn = {0178-8051, 1432-2064},
  doi = {10.1007/s00440-021-01089-7},
  url = {http://arxiv.org/abs/1908.09959},
  urldate = {2025-03-16},
  abstract = {We study support recovery for a \$k \textbackslash times k\$ principal submatrix with elevated mean \$\textbackslash lambda/N\$, hidden in an \$N\textbackslash times N\$ symmetric mean zero Gaussian matrix. Here \$\textbackslash lambda{$>$}0\$ is a universal constant, and we assume \$k = N \textbackslash rho\$ for some constant \$\textbackslash rho \textbackslash in (0,1)\$. We establish that \{there exists a constant \$C{$>$}0\$ such that\} the MLE recovers a constant proportion of the hidden submatrix if \$\textbackslash lambda \{\textbackslash geq C\} \textbackslash sqrt\{\textbackslash frac\{1\}\{\textbackslash rho\} \textbackslash log \textbackslash frac\{1\}\{\textbackslash rho\}\}\$, \{while such recovery is information theoretically impossible if \$\textbackslash lambda = o( \textbackslash sqrt\{\textbackslash frac\{1\}\{\textbackslash rho\} \textbackslash log \textbackslash frac\{1\}\{\textbackslash rho\}\} )\$\}. The MLE is computationally intractable in general, and in fact, for \$\textbackslash rho{$>$}0\$ sufficiently small, this problem is conjectured to exhibit a \textbackslash emph\{statistical-computational gap\}. To provide rigorous evidence for this, we study the likelihood landscape for this problem, and establish that for some \$\textbackslash varepsilon{$>$}0\$ and \$\textbackslash sqrt\{\textbackslash frac\{1\}\{\textbackslash rho\} \textbackslash log \textbackslash frac\{1\}\{\textbackslash rho\} \} \textbackslash ll \textbackslash lambda \textbackslash ll \textbackslash frac\{1\}\{\textbackslash rho\textasciicircum\{1/2 + \textbackslash varepsilon\}\}\$, the problem exhibits a variant of the \textbackslash emph\{Overlap-Gap-Property (OGP)\}. As a direct consequence, we establish that a family of local MCMC based algorithms do not achieve optimal recovery. Finally, we establish that for \$\textbackslash lambda {$>$} 1/\textbackslash rho\$, a simple spectral method recovers a constant proportion of the hidden submatrix.},
  keywords = {Computer Science - Computational Complexity,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/PHGR2PGE/Gamarnik et al. - 2021 - The Overlap Gap Property in Principal Submatrix Recovery.pdf;/Users/rushilma/Zotero/storage/VA8DC43K/1908.html}
}

@article{gamarnikPerformanceSequentialLocal2017,
  title = {Performance of {{Sequential Local Algorithms}} for the {{Random NAE-}}\${{K}}\$-{{SAT Problem}}},
  author = {Gamarnik, David and Sudan, Madhu},
  date = {2017-01},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  volume = {46},
  number = {2},
  pages = {590--619},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/140989728},
  url = {https://epubs.siam.org/doi/10.1137/140989728},
  urldate = {2025-03-16},
  langid = {english},
  file = {/Users/rushilma/Zotero/storage/CJRVZPGT/Gamarnik and Sudan - 2017 - Performance of Sequential Local Algorithms for the Random NAE-$K$-SAT Problem.pdf}
}

@online{gamarnikSparseHighDimensionalLinear2019,
  title = {Sparse {{High-Dimensional Linear Regression}}. {{Algorithmic Barriers}} and a {{Local Search Algorithm}}},
  author = {Gamarnik, David and Zadik, Ilias},
  date = {2019-09-22},
  eprint = {1711.04952},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1711.04952},
  url = {http://arxiv.org/abs/1711.04952},
  urldate = {2025-03-16},
  abstract = {We consider a sparse high dimensional regression model where the goal is to recover a \$k\$-sparse unknown vector \$\textbackslash beta\textasciicircum *\$ from \$n\$ noisy linear observations of the form \$Y=X\textbackslash beta\textasciicircum *+W \textbackslash in \textbackslash mathbb\{R\}\textasciicircum n\$ where \$X \textbackslash in \textbackslash mathbb\{R\}\textasciicircum\{n \textbackslash times p\}\$ has iid \$N(0,1)\$ entries and \$W \textbackslash in \textbackslash mathbb\{R\}\textasciicircum n\$ has iid \$N(0,\textbackslash sigma\textasciicircum 2)\$ entries. Under certain assumptions on the parameters, an intriguing assymptotic gap appears between the minimum value of \$n\$, call it \$n\textasciicircum *\$, for which the recovery is information theoretically possible, and the minimum value of \$n\$, call it \$n\_\{\textbackslash mathrm\{alg\}\}\$, for which an efficient algorithm is known to provably recover \$\textbackslash beta\textasciicircum *\$. In \textbackslash cite\{gamarnikzadik\} it was conjectured that the gap is not artificial, in the sense that for sample sizes \$n \textbackslash in [n\textasciicircum *,n\_\{\textbackslash mathrm\{alg\}\}]\$ the problem is algorithmically hard. We support this conjecture in two ways. Firstly, we show that the optimal solution of the LASSO provably fails to \$\textbackslash ell\_2\$-stably recover the unknown vector \$\textbackslash beta\textasciicircum *\$ when \$n \textbackslash in [n\textasciicircum *,c n\_\{\textbackslash mathrm\{alg\}\}]\$, for some sufficiently small constant \$c{$>$}0\$. Secondly, we establish that \$n\_\{\textbackslash mathrm\{alg\}\}\$, up to a multiplicative constant factor, is a phase transition point for the appearance of a certain Overlap Gap Property (OGP) over the space of \$k\$-sparse vectors. The presence of such an Overlap Gap Property phase transition, which originates in statistical physics, is known to provide evidence of an algorithmic hardness. Finally we show that if \$n{$>$}C n\_\{\textbackslash mathrm\{alg\}\}\$ for some large enough constant \$C{$>$}0\$, a very simple algorithm based on a local search improvement rule is able both to \$\textbackslash ell\_2\$-stably recover the unknown vector \$\textbackslash beta\textasciicircum *\$ and to infer correctly its support, adding it to the list of provably successful algorithms for the high dimensional linear regression problem.},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/3AAVSGQE/Gamarnik and Zadik - 2019 - Sparse High-Dimensional Linear Regression. Algorithmic Barriers and a Local Search Algorithm.pdf;/Users/rushilma/Zotero/storage/XKLL2PS9/1711.html}
}

@book{gareyComputersIntractabilityGuide1979,
  title = {Computers and {{Intractability}}: {{A Guide}} to the {{Theory}} of {{NP-Completeness}}},
  shorttitle = {Computers and Intractability},
  author = {Garey, Michael R. and Johnson, David S.},
  date = {1979},
  series = {A Series of Books in the Mathematical Sciences},
  publisher = {W. H. Freeman},
  location = {New York},
  isbn = {978-0-7167-1045-5},
  langid = {english}
}

@article{gentAnalysisHeuristicsNumber1998,
  title = {Analysis of {{Heuristics}} for {{Number Partitioning}}},
  author = {Gent, Ian P. and Walsh, Toby},
  date = {1998},
  journaltitle = {Computational Intelligence},
  volume = {14},
  number = {3},
  pages = {430--451},
  issn = {1467-8640},
  doi = {10.1111/0824-7935.00069},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/0824-7935.00069},
  urldate = {2025-03-15},
  abstract = {We illustrate the use of phase transition behavior in the study of heuristics. Using an “annealed” theory, we define a parameter that measures the “constrainedness” of an ensemble of number partitioning problems. We identify a phase transition at a critical value of constrainedness. We then show that constrainedness can be used to analyze and compare algorithms and heuristics for number partitioning in a precise and quantitative manner. For example, we demonstrate that on uniform random problems both the Karmarkar–Karp and greedy heuristics minimize the constrainedness, but that the decisions made by the Karmarkar–Karp heuristic are superior at reducing constrainedness. This supports the better performance observed experimentally for the Karmarkar–Karp heuristic. Our results refute a conjecture of Fu that phase transition behavior does not occur in number partitioning. Additionally, they demonstrate that phase transition behavior is useful for more than just simple benchmarking. It can, for instance, be used to analyze heuristics, and to compare the quality of heuristic solutions.},
  langid = {english},
  keywords = {heuristics,number partitioning,phase transitions},
  file = {/Users/rushilma/Zotero/storage/XE5KQPQS/Gent and Walsh - 1998 - Analysis of Heuristics for Number Partitioning.pdf}
}

@article{gentPhaseTransitionsAnnealed2000,
  title = {Phase {{Transitions}} and {{Annealed Theories}}: {{Number Partitioning}} as a {{Case Study}}},
  author = {Gent, Ian and Walsh, Toby},
  date = {2000-06-03},
  journaltitle = {Instituto Cultura}
}

@online{harshawBalancingCovariatesRandomized2023,
  title = {Balancing {{Covariates}} in {{Randomized Experiments}} with the {{Gram-Schmidt Walk Design}}},
  author = {Harshaw, Christopher and Sävje, Fredrik and Spielman, Daniel and Zhang, Peng},
  date = {2023-11-14},
  eprint = {1911.03071},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1911.03071},
  url = {http://arxiv.org/abs/1911.03071},
  urldate = {2025-03-15},
  abstract = {The design of experiments involves a compromise between covariate balance and robustness. This paper provides a formalization of this trade-off and describes an experimental design that allows experimenters to navigate it. The design is specified by a robustness parameter that bounds the worst-case mean squared error of an estimator of the average treatment effect. Subject to the experimenter's desired level of robustness, the design aims to simultaneously balance all linear functions of potentially many covariates. Less robustness allows for more balance. We show that the mean squared error of the estimator is bounded in finite samples by the minimum of the loss function of an implicit ridge regression of the potential outcomes on the covariates. Asymptotically, the design perfectly balances all linear functions of a growing number of covariates with a diminishing reduction in robustness, effectively allowing experimenters to escape the compromise between balance and robustness in large samples. Finally, we describe conditions that ensure asymptotic normality and provide a conservative variance estimator, which facilitate the construction of asymptotically valid confidence intervals.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Statistics Theory,Statistics - Methodology,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/D7PXMQZE/Harshaw et al. - 2023 - Balancing Covariates in Randomized Experiments with the Gram-Schmidt Walk Design.pdf;/Users/rushilma/Zotero/storage/VH4R843P/1911.html}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-0-387-84858-7},
  url = {http://link.springer.com/10.1007/978-0-387-84858-7},
  urldate = {2024-12-28},
  isbn = {978-0-387-84857-0 978-0-387-84858-7}
}

@article{hatamiLimitsLocallyGlobally2014,
  title = {Limits of Locally–Globally Convergent Graph Sequences},
  author = {Hatami, Hamed and Lovász, László and Szegedy, Balázs},
  date = {2014-02},
  journaltitle = {Geometric and Functional Analysis},
  shortjournal = {Geom. Funct. Anal.},
  volume = {24},
  number = {1},
  pages = {269--296},
  issn = {1016-443X, 1420-8970},
  doi = {10.1007/s00039-014-0258-7},
  url = {http://link.springer.com/10.1007/s00039-014-0258-7},
  urldate = {2025-03-16},
  langid = {english}
}

@online{hobergNumberBalancingHard2016,
  title = {Number {{Balancing}} Is as Hard as {{Minkowski}}'s {{Theorem}} and {{Shortest Vector}}},
  author = {Hoberg, Rebecca and Ramadas, Harishchandra and Rothvoss, Thomas and Yang, Xin},
  date = {2016-11-29},
  eprint = {1611.08757},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1611.08757},
  url = {http://arxiv.org/abs/1611.08757},
  urldate = {2025-03-15},
  abstract = {The number balancing (NBP) problem is the following: given real numbers a1, . . . , an ∈ [0, 1], find two disjoint subsets I1, I2 ⊆ [n] so that the difference | ∑ i∈I1 ai − ∑ i∈I2 ai| of their sums is minimized.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Computational Geometry,Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics},
  file = {/Users/rushilma/Zotero/storage/98M9KHH6/Hoberg et al. - 2016 - Number Balancing is as hard as Minkowski's Theorem and Shortest Vector.pdf}
}

@online{hopkinsPowerSumofsquaresDetecting2017,
  title = {The Power of Sum-of-Squares for Detecting Hidden Structures},
  author = {Hopkins, Samuel B. and Kothari, Pravesh K. and Potechin, Aaron and Raghavendra, Prasad and Schramm, Tselil and Steurer, David},
  date = {2017-10-13},
  eprint = {1710.05017},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.05017},
  url = {http://arxiv.org/abs/1710.05017},
  urldate = {2025-03-16},
  abstract = {We study planted problems---finding hidden structures in random noisy inputs---through the lens of the sum-of-squares semidefinite programming hierarchy (SoS). This family of powerful semidefinite programs has recently yielded many new algorithms for planted problems, often achieving the best known polynomial-time guarantees in terms of accuracy of recovered solutions and robustness to noise. One theme in recent work is the design of spectral algorithms which match the guarantees of SoS algorithms for planted problems. Classical spectral algorithms are often unable to accomplish this: the twist in these new spectral algorithms is the use of spectral structure of matrices whose entries are low-degree polynomials of the input variables. We prove that for a wide class of planted problems, including refuting random constraint satisfaction problems, tensor and sparse PCA, densest-k-subgraph, community detection in stochastic block models, planted clique, and others, eigenvalues of degree-d matrix polynomials are as powerful as SoS semidefinite programs of roughly degree d. For such problems it is therefore always possible to match the guarantees of SoS without solving a large semidefinite program. Using related ideas on SoS algorithms and low-degree matrix polynomials (and inspired by recent work on SoS and the planted clique problem by Barak et al.), we prove new nearly-tight SoS lower bounds for the tensor and sparse principal component analysis problems. Our lower bounds for sparse principal component analysis are the first to suggest that going beyond existing algorithms for this problem may require sub-exponential time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms},
  file = {/Users/rushilma/Zotero/storage/TXUCCKND/Hopkins et al. - 2017 - The power of sum-of-squares for detecting hidden structures.pdf;/Users/rushilma/Zotero/storage/FNP7VSF3/1710.html}
}

@thesis{hopkinsStatisticalInferenceSum2018,
  title = {Statistical {{Inference}} and the {{Sum}} of {{Squares Method}}},
  author = {Hopkins, Samuel},
  date = {2018-08},
  institution = {Cornell University}
}

@online{hopkinsTensorPrincipalComponent2015,
  title = {Tensor Principal Component Analysis via Sum-of-Squares Proofs},
  author = {Hopkins, Samuel B. and Shi, Jonathan and Steurer, David},
  date = {2015-07-12},
  eprint = {1507.03269},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1507.03269},
  url = {http://arxiv.org/abs/1507.03269},
  urldate = {2025-03-16},
  abstract = {We study a statistical model for the tensor principal component analysis problem introduced by Montanari and Richard: Given a order-\$3\$ tensor \$T\$ of the form \$T = \textbackslash tau \textbackslash cdot v\_0\textasciicircum\{\textbackslash otimes 3\} + A\$, where \$\textbackslash tau \textbackslash geq 0\$ is a signal-to-noise ratio, \$v\_0\$ is a unit vector, and \$A\$ is a random noise tensor, the goal is to recover the planted vector \$v\_0\$. For the case that \$A\$ has iid standard Gaussian entries, we give an efficient algorithm to recover \$v\_0\$ whenever \$\textbackslash tau \textbackslash geq \textbackslash omega(n\textasciicircum\{3/4\} \textbackslash log(n)\textasciicircum\{1/4\})\$, and certify that the recovered vector is close to a maximum likelihood estimator, all with high probability over the random choice of \$A\$. The previous best algorithms with provable guarantees required \$\textbackslash tau \textbackslash geq \textbackslash Omega(n)\$. In the regime \$\textbackslash tau \textbackslash leq o(n)\$, natural tensor-unfolding-based spectral relaxations for the underlying optimization problem break down (in the sense that their integrality gap is large). To go beyond this barrier, we use convex relaxations based on the sum-of-squares method. Our recovery algorithm proceeds by rounding a degree-\$4\$ sum-of-squares relaxations of the maximum-likelihood-estimation problem for the statistical model. To complement our algorithmic results, we show that degree-\$4\$ sum-of-squares relaxations break down for \$\textbackslash tau \textbackslash leq O(n\textasciicircum\{3/4\}/\textbackslash log(n)\textasciicircum\{1/4\})\$, which demonstrates that improving our current guarantees (by more than logarithmic factors) would require new techniques or might even be intractable. Finally, we show how to exploit additional problem structure in order to solve our sum-of-squares relaxations, up to some approximation, very efficiently. Our fastest algorithm runs in nearly-linear time using shifted (matrix) power iteration and has similar guarantees as above. The analysis of this algorithm also confirms a variant of a conjecture of Montanari and Richard about singular vectors of tensor unfoldings.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/M5QYQG92/Hopkins et al. - 2015 - Tensor principal component analysis via sum-of-squares proofs.pdf}
}

@online{huangOptimalLowDegree2025,
  title = {Optimal {{Low}} Degree Hardness for {{Broadcasting}} on {{Trees}}},
  author = {Huang, Han and Mossel, Elchanan},
  date = {2025-02-07},
  eprint = {2502.04861},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2502.04861},
  url = {http://arxiv.org/abs/2502.04861},
  urldate = {2025-03-26},
  abstract = {Broadcasting on trees is a fundamental model from statistical physics that plays an important role in information theory, noisy computation and phylogenetic reconstruction within computational biology and linguistics. While this model permits efficient linear-time algorithms for the inference of the root from the leaves, recent work suggests that non-trivial computational complexity may be required for inference. The inference of the root state can be performed using the celebrated Belief Propagation (BP) algorithm, which achieves Bayes-optimal performance. Although BP runs in linear time using real arithmetic operations, recent research indicates that it requires non-trivial computational complexity using more refined complexity measures. Moitra, Mossel, and Sandon demonstrated such complexity by constructing a Markov chain for which estimating the root better than random guessing (for typical inputs) is \$NC\textasciicircum 1\$-complete. Kohler and Mossel constructed chains where, for trees with \$N\$ leaves, achieving better-than-random root recovery requires polynomials of degree \$N\textasciicircum\{\textbackslash Omega(1)\}\$. The papers above raised the question of whether such complexity bounds hold generally below the celebrated Kesten-Stigum bound. In a recent work, Huang and Mossel established a general degree lower bound of \$\textbackslash Omega(\textbackslash log N)\$ below the Kesten-Stigum bound. Specifically, they proved that any function expressed as a linear combination of functions of at most \$O(log N)\$ leaves has vanishing correlation with the root. In this work, we get an exponential improvement of this lower bound by establishing an \$N\textasciicircum\{\textbackslash Omega(1)\}\$ degree lower bound, for any broadcast process in the whole regime below the Kesten-Stigum bound.},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/H64A2AWS/2502.html}
}

@online{huangStrongLowDegree2025,
  title = {Strong {{Low Degree Hardness}} for {{Stable Local Optima}} in {{Spin Glasses}}},
  author = {Huang, Brice and Sellke, Mark},
  date = {2025-01-11},
  eprint = {2501.06427},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  doi = {10.48550/arXiv.2501.06427},
  url = {http://arxiv.org/abs/2501.06427},
  urldate = {2025-01-30},
  abstract = {It is a folklore belief in the theory of spin glasses and disordered systems that out-of-equilibrium dynamics fail to find stable local optima exhibiting e.g. local strict convexity on physical time-scales. In the context of the Sherrington--Kirkpatrick spin glass, Behrens-Arpino-Kivva-Zdeborov\textbackslash 'a and Minzer-Sah-Sawhney have recently conjectured that this obstruction may be inherent to all efficient algorithms, despite the existence of exponentially many such optima throughout the landscape. We prove this search problem exhibits strong low degree hardness for polynomial algorithms of degree \$D\textbackslash leq o(N)\$: any such algorithm has probability \$o(1)\$ to output a stable local optimum. To the best of our knowledge, this is the first result to prove that even constant-degree polynomials have probability \$o(1)\$ to solve a random search problem without planted structure. To prove this, we develop a general-purpose enhancement of the ensemble overlap gap property, and as a byproduct improve previous results on spin glass optimization, maximum independent set, random \$k\$-SAT, and the Ising perceptron to strong low degree hardness. Finally for spherical spin glasses with no external field, we prove that Langevin dynamics does not find stable local optima within dimension-free time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Condensed Matter - Disordered Systems and Neural Networks,Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/Y2BFEEYK/Huang and Sellke - 2025 - Strong Low Degree Hardness for Stable Local Optima in Spin Glasses.pdf;/Users/rushilma/Zotero/storage/6T67KDNH/2501.html}
}

@article{jerrumLargeCliquesElude1992,
  title = {Large {{Cliques Elude}} the {{Metropolis Process}}},
  author = {Jerrum, Mark},
  date = {1992-01},
  journaltitle = {Random Structures \& Algorithms},
  shortjournal = {Random Struct Algorithms},
  volume = {3},
  number = {4},
  pages = {347--359},
  issn = {1042-9832, 1098-2418},
  doi = {10.1002/rsa.3240030402},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/rsa.3240030402},
  urldate = {2025-03-16},
  abstract = {Abstract                            In a random graph on               n               vertices, the maximum clique is likely to be of size very close to 2 lg               n               . However, the clique produced by applying the naive “greedy” heuristic to a random graph is unlikely to have size much exceeding lg               n               . The factor of two separating these estimates motivates the search for more effective heuristics. This article analyzes a heuristic search strategy, the               Metropolis process               , which is just one step above the greedy one in its level of sophistication. It is shown that the Metropolis process takes super‐polynomial time to locate a clique that is only slightly bigger than that produced by the greedy heuristic.},
  langid = {english}
}

@article{johnsonOptimizationSimulatedAnnealing1989b,
  title = {Optimization by {{Simulated Annealing}}: {{An Experimental Evaluation}}; {{Part I}}, {{Graph Partitioning}}},
  author = {Johnson, David S. and Aragon, Cecilia R. and McGeoch, Lyle A. and Schevon, Catherine},
  date = {1989},
  journaltitle = {Operations Research},
  volume = {37},
  number = {6},
  eprint = {171470},
  eprinttype = {jstor},
  pages = {865--892},
  publisher = {INFORMS},
  issn = {0030364X, 15265463},
  url = {http://www.jstor.org/stable/171470},
  urldate = {2025-03-15},
  abstract = {[In this and two companion papers, we report on an extended empirical study of the simulated annealing approach to combinatorial optimization proposed by S. Kirkpatrick et al. That study investigated how best to adapt simulated annealing to particular problems and compared its performance to that of more traditional algorithms. This paper (Part I) discusses annealing and our parameterized generic implementation of it, describes how we adapted this generic algorithm to the graph partitioning problem, and reports how well it compared to standard algorithms like the Kernighan-Lin algorithm. (For sparse random graphs, it tended to outperform Kernighan-Lin as the number of vertices become large, even when its much greater running time was taken into account. It did not perform nearly so well, however, on graphs generated with a built-in geometric structure.) We also discuss how we went about optimizing our implementation, and describe the effects of changing the various annealing parameters or varying the basic annealing algorithm itself.]}
}

@article{johnsonOptimizationSimulatedAnnealing1991,
  title = {Optimization by {{Simulated Annealing}}: {{An Experimental Evaluation}}; {{Part II}}, {{Graph Coloring}} and {{Number Partitioning}}},
  author = {Johnson, David S. and Aragon, Cecilia R. and McGeoch, Lyle A. and Schevon, Catherine},
  date = {1991},
  journaltitle = {Operations Research},
  volume = {39},
  number = {3},
  eprint = {171393},
  eprinttype = {jstor},
  pages = {378--406},
  publisher = {INFORMS},
  issn = {0030364X, 15265463},
  url = {http://www.jstor.org/stable/171393},
  urldate = {2025-03-15},
  abstract = {[This is the second in a series of three papers that empirically examine the competitiveness of simulated annealing in certain well-studied domains of combinatorial optimization. Simulated annealing is a randomized technique proposed by S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi for improving local optimization algorithms. Here we report on experiments at adapting simulated annealing to graph coloring and number partitioning, two problems for which local optimization had not previously been thought suitable. For graph coloring, we report on three simulated annealing schemes, all of which can dominate traditional techniques for certain types of graphs, at least when large amounts of computing time are available. For number partitioning, simulated annealing is not competitive with the differencing algorithm of N. Karmarkar and R. M. Karp, except on relatively small instances. Moreover, if running time is taken into account, natural annealing schemes cannot even outperform multiple random runs of the local optimization algorithms on which they are based, in sharp contrast to the observed performance of annealing on other problems.]}
}

@article{karmarkarProbabilisticAnalysisOptimum1986,
  title = {Probabilistic {{Analysis}} of {{Optimum Partitioning}}},
  author = {Karmarkar, Narendra and Karp, Richard M. and Lueker, George S. and Odlyzko, Andrew M.},
  date = {1986},
  journaltitle = {Journal of Applied Probability},
  volume = {23},
  number = {3},
  eprint = {3214002},
  eprinttype = {jstor},
  pages = {626--645},
  publisher = {Applied Probability Trust},
  issn = {0021-9002},
  doi = {10.2307/3214002},
  url = {https://www.jstor.org/stable/3214002},
  urldate = {2025-03-10},
  abstract = {Given a set of n items with real-valued sizes, the optimum partition problem asks how it can be partitioned into two subsets so that the absolute value of the difference of the sums of the sizes over the two subsets is minimized. We present bounds on the probability distribution of this minimum under the assumption that the sizes are independent random variables drawn from a common distribution. For a large class of distributions, we determine the asymptotic behavior of the median of this minimum, and show that it is exponentially small.},
  file = {/Users/rushilma/Zotero/storage/JP2G3AR7/Karmarkar et al. - 1986 - Probabilistic Analysis of Optimum Partitioning.pdf}
}

@report{karmarkerDifferencingMethodSet1983,
  title = {The {{Differencing Method}} of {{Set Partitioning}}},
  author = {Karmarker, Narendra and Karp, Richard M.},
  date = {1983},
  number = {UCB/CSD-83-113},
  url = {https://www2.eecs.berkeley.edu/Pubs/TechRpts/1983/6353.html},
  urldate = {2025-03-15},
  file = {/Users/rushilma/Zotero/storage/CDCP7QF2/6353.html}
}

@article{kearnsEfficientNoisetolerantLearning1998,
  title = {Efficient Noise-Tolerant Learning from Statistical Queries},
  author = {Kearns, Michael},
  date = {1998-11},
  journaltitle = {Journal of the ACM},
  shortjournal = {J. ACM},
  volume = {45},
  number = {6},
  pages = {983--1006},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/293347.293351},
  url = {https://dl.acm.org/doi/10.1145/293347.293351},
  urldate = {2025-03-16},
  abstract = {In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of “robust” learning algorithms in the most general way, we formalize a new but related model of learning from               statistical queries               . Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.                          One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a  noise rate approaching the  information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.},
  langid = {english},
  file = {/Users/rushilma/Zotero/storage/ARVLU8YK/Kearns - 1998 - Efficient noise-tolerant learning from statistical queries.pdf}
}

@online{kizildagPlantedRandomNumber2023,
  title = {Planted {{Random Number Partitioning Problem}}},
  author = {Kızıldağ, Eren C.},
  date = {2023-09-26},
  eprint = {2309.15115},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2309.15115},
  url = {http://arxiv.org/abs/2309.15115},
  urldate = {2025-03-15},
  abstract = {We consider the random number partitioning problem (\textbackslash texttt\{NPP\}): given a list \$X\textbackslash sim \textbackslash mathcal\{N\}(0,I\_n)\$ of numbers, find a partition \$\textbackslash sigma\textbackslash in\textbackslash\{-1,1\textbackslash\}\textasciicircum n\$ with a small objective value \$H(\textbackslash sigma)=\textbackslash frac\{1\}\{\textbackslash sqrt\{n\}\}\textbackslash left|\textbackslash langle \textbackslash sigma,X\textbackslash rangle\textbackslash right|\$. The \textbackslash texttt\{NPP\} is widely studied in computer science; it is also closely related to the design of randomized controlled trials. In this paper, we propose a planted version of the \textbackslash texttt\{NPP\}: fix a \$\textbackslash sigma\textasciicircum *\$ and generate \$X\textbackslash sim \textbackslash mathcal\{N\}(0,I\_n)\$ conditional on \$H(\textbackslash sigma\textasciicircum *)\textbackslash le 3\textasciicircum\{-n\}\$. The \textbackslash texttt\{NPP\} and its planted counterpart are statistically distinguishable as the smallest objective value under the former is \$\textbackslash Theta(\textbackslash sqrt\{n\}2\textasciicircum\{-n\})\$ w.h.p. Our first focus is on the values of \$H(\textbackslash sigma)\$. We show that, perhaps surprisingly, planting does not induce partitions with an objective value substantially smaller than \$2\textasciicircum\{-n\}\$: \$\textbackslash min\_\{\textbackslash sigma \textbackslash ne \textbackslash pm \textbackslash sigma\textasciicircum *\}H(\textbackslash sigma) = \textbackslash widetilde\{\textbackslash Theta\}(2\textasciicircum\{-n\})\$ w.h.p. Furthermore, we completely characterize the smallest \$H(\textbackslash sigma)\$ achieved at any fixed distance from \$\textbackslash sigma\textasciicircum *\$. Our second focus is on the algorithmic problem of efficiently finding a partition \$\textbackslash sigma\$, not necessarily equal to \$\textbackslash pm\textbackslash sigma\textasciicircum *\$, with a small \$H(\textbackslash sigma)\$. We show that planted \textbackslash texttt\{NPP\} exhibits an intricate geometrical property known as the multi Overlap Gap Property (\$m\$-OGP) for values \$2\textasciicircum\{-\textbackslash Theta(n)\}\$. We then leverage the \$m\$-OGP to show that stable algorithms satisfying a certain anti-concentration property fail to find a \$\textbackslash sigma\$ with \$H(\textbackslash sigma)=2\textasciicircum\{-\textbackslash Theta(n)\}\$. Our results are the first instance of the \$m\$-OGP being established and leveraged to rule out stable algorithms for a planted model. More importantly, they show that the \$m\$-OGP framework can also apply to planted models, if the algorithmic goal is to return a solution with a small objective value.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/GJ5QMX8R/Kızıldağ - 2023 - Planted Random Number Partitioning Problem.pdf;/Users/rushilma/Zotero/storage/GC4UIBMK/2309.html}
}

@article{kojicIntegerLinearProgramming2010,
  title = {Integer Linear Programming Model for Multidimensional Two-Way Number Partitioning Problem},
  author = {Kojić, Jelena},
  date = {2010-10},
  journaltitle = {Computers \& Mathematics with Applications},
  shortjournal = {Computers \& Mathematics with Applications},
  volume = {60},
  number = {8},
  pages = {2302--2308},
  issn = {08981221},
  doi = {10.1016/j.camwa.2010.08.024},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0898122110005882},
  urldate = {2025-03-15},
  langid = {english}
}

@inproceedings{korfApproximateOptimalSolutions1995,
  title = {From Approximate to Optimal Solutions: A Case Study of Number Partitioning},
  shorttitle = {From Approximate to Optimal Solutions},
  booktitle = {Proceedings of the 14th International Joint Conference on {{Artificial}} Intelligence - {{Volume}} 1},
  author = {Korf, Richard E.},
  date = {1995-08-20},
  series = {{{IJCAI}}'95},
  pages = {266--272},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  abstract = {Given a set of numbers, the two-way partitioning problem is to divide them into two subsets, so that the sum of the numbers in each subset are as nearly equal as possible. The problem is NP-complete, and is contained in many scheduling applications. Based on a polynomial-time heuristic due to Karmarkar and Karp, we present a new algorithm, called Complete Karmarkar Karp (CKK), that optimally solves the general number-partitioning problem. CKK significantly outperforms the best previously-known algorithms for this problem. By restricting the numbers to twelve significant digits, we can optimally solve two-way partitioning problems of arbitrary size in practice. CKK first returns the Karmarkar-Karp solution, then continues to find better solutions as time allows. Almost five orders of magnitude improvement in solution quality is obtained within a minute of running time. Rather than building a single solution one element at a time, CKK constructs subsolutions, and combines them in all possible ways. CKK is directly applicable to the 0/1 knapsack problem, since it can be reduced to number partitioning. This general approach may also be applicable to other NP-hard problems as well.},
  isbn = {978-1-55860-363-9}
}

@article{korfCompleteAnytimeAlgorithm1998,
  title = {A Complete Anytime Algorithm for Number Partitioning},
  author = {Korf, Richard E.},
  date = {1998-12},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {106},
  number = {2},
  pages = {181--203},
  issn = {00043702},
  doi = {10.1016/S0004-3702(98)00086-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370298000861},
  urldate = {2025-03-15},
  langid = {english}
}

@inproceedings{korfMultiwayNumberPartitioning2009,
  title = {Multi-Way Number Partitioning},
  booktitle = {Proceedings of the 21st {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Korf, Richard E.},
  date = {2009-07-11},
  series = {{{IJCAI}}'09},
  pages = {538--543},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  abstract = {The number partitioning problem is to divide a given set of integers into a collection of subsets, so that the sum of the numbers in each subset are as nearly equal as possible. While a very efficient algorithm exists for optimal two-way partitioning, it is not nearly as effective for multi-way partitioning. We develop two new linear-space algorithms for multi-way partitioning, and demonstrate their performance on three, four, and five-way partitioning. In each case, our algorithms outperform the previous state of the art by orders of magnitude, in one case by over six orders of magnitude. Empirical analysis of the running times of our algorithms strongly suggest that their asymptotic growth is less than that of previous algorithms. The key insight behind both our new algorithms is that if an optimal k-way partition includes a particular subset, then optimally partitioning the numbers not in that set k-1 ways results in an optimal k-way partition.}
}

@online{kothariSumSquaresLower2017,
  title = {Sum of Squares Lower Bounds for Refuting Any {{CSP}}},
  author = {Kothari, Pravesh K. and Mori, Ryuhei and O'Donnell, Ryan and Witmer, David},
  date = {2017-01-17},
  eprint = {1701.04521},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1701.04521},
  url = {http://arxiv.org/abs/1701.04521},
  urldate = {2025-03-16},
  abstract = {Let \$P:\textbackslash\{0,1\textbackslash\}\textasciicircum k \textbackslash to \textbackslash\{0,1\textbackslash\}\$ be a nontrivial \$k\$-ary predicate. Consider a random instance of the constraint satisfaction problem \$\textbackslash mathrm\{CSP\}(P)\$ on \$n\$ variables with \$\textbackslash Delta n\$ constraints, each being \$P\$ applied to \$k\$ randomly chosen literals. Provided the constraint density satisfies \$\textbackslash Delta \textbackslash gg 1\$, such an instance is unsatisfiable with high probability. The \textbackslash emph\{refutation\} problem is to efficiently find a proof of unsatisfiability. We show that whenever the predicate \$P\$ supports a \$t\$-\textbackslash emph\{wise uniform\} probability distribution on its satisfying assignments, the sum of squares (SOS) algorithm of degree \$d = \textbackslash Theta(\textbackslash frac\{n\}\{\textbackslash Delta\textasciicircum\{2/(t-1)\} \textbackslash log \textbackslash Delta\})\$ (which runs in time \$n\textasciicircum\{O(d)\}\$) \textbackslash emph\{cannot\} refute a random instance of \$\textbackslash mathrm\{CSP\}(P)\$. In particular, the polynomial-time SOS algorithm requires \$\textbackslash widetilde\{\textbackslash Omega\}(n\textasciicircum\{(t+1)/2\})\$ constraints to refute random instances of CSP\$(P)\$ when \$P\$ supports a \$t\$-wise uniform distribution on its satisfying assignments. Together with recent work of Lee et al. [LRS15], our result also implies that \textbackslash emph\{any\} polynomial-size semidefinite programming relaxation for refutation requires at least \$\textbackslash widetilde\{\textbackslash Omega\}(n\textasciicircum\{(t+1)/2\})\$ constraints. Our results (which also extend with no change to CSPs over larger alphabets) subsume all previously known lower bounds for semialgebraic refutation of random CSPs. For every constraint predicate\textasciitilde\$P\$, they give a three-way hardness tradeoff between the density of constraints, the SOS degree (hence running time), and the strength of the refutation. By recent algorithmic results of Allen et al. [AOW15] and Raghavendra et al. [RRS16], this full three-way tradeoff is \textbackslash emph\{tight\}, up to lower-order factors.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/rushilma/Zotero/storage/34334YAA/Kothari et al. - 2017 - Sum of squares lower bounds for refuting any CSP.pdf;/Users/rushilma/Zotero/storage/FM8Y3FAF/1701.html}
}

@article{kraticaTwoMetaheuristicApproaches2014,
  title = {Two Metaheuristic Approaches for Solving Multidimensional Two-Way Number Partitioning Problem},
  author = {Kratica, Jozef and Kojić, Jelena and Savić, Aleksandar},
  date = {2014-06},
  journaltitle = {Computers \& Operations Research},
  shortjournal = {Computers \& Operations Research},
  volume = {46},
  pages = {59--68},
  issn = {03050548},
  doi = {10.1016/j.cor.2014.01.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054814000045},
  urldate = {2025-03-15},
  langid = {english}
}

@article{kriegerNearlyRandomDesigns2019,
  title = {Nearly Random Designs with Greatly Improved Balance},
  author = {Krieger, A M and Azriel, D and Kapelner, A},
  date = {2019-09-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {106},
  number = {3},
  pages = {695--701},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz026},
  url = {https://doi.org/10.1093/biomet/asz026},
  urldate = {2025-03-15},
  abstract = {We present a procedure that divides a set of experimental units into two groups that are similar on a prespecified set of covariates and are almost as random as with a complete randomization. Under complete randomization, the difference in the standardized average between treatment and control is \$O\_\{\textbackslash rm p\}(n\textasciicircum\{-1/2\})\$, which may be material in small samples. We present an algorithm that reduces imbalance to \$O\_\{\textbackslash rm p\}(n\textasciicircum\{-3\})\$ for one covariate and \$O\_\{\textbackslash rm p\}\textbackslash\{n\textasciicircum\{-(1 + 2/p)\}\textbackslash\}\$ for \$p\$ covariates, but whose assignments are, strictly speaking, nonrandom. In addition to the metric of maximum eigenvalue of allocation variance, we introduce two metrics that capture departures from randomization and show that our assignments are nearly as random as complete randomization in terms of all measures. Simulations illustrate the results, and inference is discussed. An R package to generate designs according to our algorithm and other popular designs is available.},
  file = {/Users/rushilma/Zotero/storage/UQP6UN9H/Krieger et al. - 2019 - Nearly random designs with greatly improved balance.pdf;/Users/rushilma/Zotero/storage/E4G2X2NK/5499316.html}
}

@online{kuniskyLowCoordinateDegree2024,
  title = {Low Coordinate Degree Algorithms {{II}}: {{Categorical}} Signals and Generalized Stochastic Block Models},
  shorttitle = {Low Coordinate Degree Algorithms {{II}}},
  author = {Kunisky, Dmitriy},
  date = {2024-12-30},
  eprint = {2412.21155},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2412.21155},
  url = {http://arxiv.org/abs/2412.21155},
  urldate = {2025-03-26},
  abstract = {We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can test for the presence of categorical structure, including community structure and generalizations thereof, in high-dimensional data. This complements the first paper of this series, which studied the power of LCDF in testing for continuous structure like real-valued signals perturbed by additive noise. We apply the tools developed there to a general form of stochastic block model (SBM), where a population is assigned random labels and every \$p\$-tuple of the population generates an observation according to an arbitrary probability measure associated to the \$p\$ labels of its members. We show that the performance of LCDF admits a unified analysis for this class of models. As applications, we prove tight lower bounds against LCDF (and therefore also against low degree polynomials) for nearly arbitrary graph and regular hypergraph SBMs, always matching suitable generalizations of the Kesten-Stigum threshold. We also prove tight lower bounds for group synchronization and abelian group sumset problems under the "truth-or-Haar" noise model, and use our technical results to give an improved analysis of Gaussian multi-frequency group synchronization. In most of these models, for some parameter settings our lower bounds give new evidence for conjectural statistical-to-computational gaps. Finally, interpreting some of our findings, we propose a precise analogy between categorical and continuous signals: a general SBM as above behaves, in terms of the tradeoff between subexponential runtime cost of testing algorithms and the signal strength needed for a testing algorithm to succeed, like a spiked \$p\_*\$-tensor model of a certain order \$p\_*\$ that may be computed from the parameters of the SBM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/4YGHC5VS/Kunisky - 2024 - Low coordinate degree algorithms II Categorical signals and generalized stochastic block models.pdf;/Users/rushilma/Zotero/storage/XUYDELLN/2412.html}
}

@online{kuniskyLowCoordinateDegree2024a,
  title = {Low Coordinate Degree Algorithms {{I}}: {{Universality}} of Computational Thresholds for Hypothesis Testing},
  shorttitle = {Low Coordinate Degree Algorithms {{I}}},
  author = {Kunisky, Dmitriy},
  date = {2024-03-12},
  eprint = {2403.07862},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2403.07862},
  url = {http://arxiv.org/abs/2403.07862},
  urldate = {2025-03-26},
  abstract = {We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures. These functions are a generalization, proposed in Hopkins' 2018 thesis but seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization. Instead of the orthogonal polynomial decompositions used in LDP calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable. By way of illustration, we prove channel universality for the success of LCDF in testing for the presence of sufficiently "dilute" random signals through noisy channels: the efficacy of LCDF depends on the channel only through the scalar Fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families. As applications, we extend lower bounds against LDP for spiked matrix and tensor models under additive Gaussian noise to lower bounds against LCDF under general noisy channels. We also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations. These results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/JFNVYDGZ/Kunisky - 2024 - Low coordinate degree algorithms I Universality of computational thresholds for hypothesis testing.pdf;/Users/rushilma/Zotero/storage/5VNLNNNE/2403.html}
}

@online{kuniskyNotesComputationalHardness2019,
  title = {Notes on {{Computational Hardness}} of {{Hypothesis Testing}}: {{Predictions}} Using the {{Low-Degree Likelihood Ratio}}},
  shorttitle = {Notes on {{Computational Hardness}} of {{Hypothesis Testing}}},
  author = {Kunisky, Dmitriy and Wein, Alexander S. and Bandeira, Afonso S.},
  date = {2019-07-26},
  eprint = {1907.11636},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1907.11636},
  url = {http://arxiv.org/abs/1907.11636},
  urldate = {2025-03-16},
  abstract = {These notes survey and explore an emerging method, which we call the low-degree method, for predicting and understanding statistical-versus-computational tradeoffs in high-dimensional inference problems. In short, the method posits that a certain quantity – the second moment of the low-degree likelihood ratio – gives insight into how much computational time is required to solve a given hypothesis testing problem, which can in turn be used to predict the computational hardness of a variety of statistical inference tasks. While this method originated in the study of the sum-of-squares (SoS) hierarchy of convex programs, we present a self-contained introduction that does not require knowledge of SoS. In addition to showing how to carry out predictions using the method, we include a discussion investigating both rigorous and conjectural consequences of these predictions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/ZJU4AEQW/Kunisky et al. - 2019 - Notes on Computational Hardness of Hypothesis Testing Predictions using the Low-Degree Likelihood R.pdf}
}

@article{lauerLargeIndependentSets2007,
  title = {Large Independent Sets in Regular Graphs of Large Girth},
  author = {Lauer, Joseph and Wormald, Nicholas},
  date = {2007-11},
  journaltitle = {Journal of Combinatorial Theory, Series B},
  shortjournal = {Journal of Combinatorial Theory, Series B},
  volume = {97},
  number = {6},
  pages = {999--1009},
  issn = {00958956},
  doi = {10.1016/j.jctb.2007.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S009589560700024X},
  urldate = {2025-03-16},
  langid = {english}
}

@inproceedings{lesieurMMSEProbabilisticLowrank2015,
  title = {{{MMSE}} of Probabilistic Low-Rank Matrix Estimation: {{Universality}} with Respect to the Output Channel},
  shorttitle = {{{MMSE}} of Probabilistic Low-Rank Matrix Estimation},
  booktitle = {2015 53rd {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  author = {Lesieur, Thibault and Krzakala, Florent and Zdeborová, Lenka},
  date = {2015-09},
  eprint = {1507.03857},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {680--687},
  doi = {10.1109/ALLERTON.2015.7447070},
  url = {http://arxiv.org/abs/1507.03857},
  urldate = {2025-03-29},
  abstract = {This paper considers probabilistic estimation of a low-rank matrix from non-linear element-wise measurements of its elements. We derive the corresponding approximate message passing (AMP) algorithm and its state evolution. Relying on non-rigorous but standard assumptions motivated by statistical physics, we characterize the minimum mean squared error (MMSE) achievable information theoretically and with the AMP algorithm. Unlike in related problems of linear estimation, in the present setting the MMSE depends on the output channel only trough a single parameter - its Fisher information. We illustrate this striking finding by analysis of submatrix localization, and of detection of communities hidden in a dense stochastic block model. For this example we locate the computational and statistical boundaries that are not equal for rank larger than four.},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/CXPBU7U2/Lesieur et al. - 2015 - MMSE of probabilistic low-rank matrix estimation Universality with respect to the output channel.pdf;/Users/rushilma/Zotero/storage/UNT9FW42/1507.html}
}

@inproceedings{lesieurPhaseTransitionsSparse2015,
  title = {Phase {{Transitions}} in {{Sparse PCA}}},
  booktitle = {2015 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  author = {Lesieur, Thibault and Krzakala, Florent and Zdeborova, Lenka},
  date = {2015-06},
  eprint = {1503.00338},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1635--1639},
  doi = {10.1109/ISIT.2015.7282733},
  url = {http://arxiv.org/abs/1503.00338},
  urldate = {2025-03-29},
  abstract = {We study optimal estimation for sparse principal component analysis when the number of non-zero elements is small but on the same order as the dimension of the data. We employ approximate message passing (AMP) algorithm and its state evolution to analyze what is the information theoretically minimal mean-squared error and the one achieved by AMP in the limit of large sizes. For a special case of rank one and large enough density of non-zeros Deshpande and Montanari [1] proved that AMP is asymptotically optimal. We show that both for low density and for large rank the problem undergoes a series of phase transitions suggesting existence of a region of parameters where estimation is information theoretically possible, but AMP (and presumably every other polynomial algorithm) fails. The analysis of the large rank limit is particularly instructive.},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/85335ZE9/Lesieur et al. - 2015 - Phase Transitions in Sparse PCA.pdf;/Users/rushilma/Zotero/storage/DMQ45ZD3/1503.html}
}

@online{levyDeterministicDiscrepancyMinimization2017,
  title = {Deterministic {{Discrepancy Minimization}} via the {{Multiplicative Weight Update Method}}},
  author = {Levy, Avi and Ramadas, Harishchandra and Rothvoss, Thomas},
  date = {2017-03-11},
  eprint = {1611.08752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1611.08752},
  url = {http://arxiv.org/abs/1611.08752},
  urldate = {2025-03-16},
  abstract = {A well-known theorem of Spencer shows that any set system with \$n\$ sets over \$n\$ elements admits a coloring of discrepancy \$O(\textbackslash sqrt\{n\})\$. While the original proof was non-constructive, recent progress brought polynomial time algorithms by Bansal, Lovett and Meka, and Rothvoss. All those algorithms are randomized, even though Bansal's algorithm admitted a complicated derandomization. We propose an elegant deterministic polynomial time algorithm that is inspired by Lovett-Meka as well as the Multiplicative Weight Update method. The algorithm iteratively updates a fractional coloring while controlling the exponential weights that are assigned to the set constraints. A conjecture by Meka suggests that Spencer's bound can be generalized to symmetric matrices. We prove that \$n \textbackslash times n\$ matrices that are block diagonal with block size \$q\$ admit a coloring of discrepancy \$O(\textbackslash sqrt\{n\} \textbackslash cdot \textbackslash sqrt\{\textbackslash log(q)\})\$. Bansal, Dadush and Garg recently gave a randomized algorithm to find a vector \$x\$ with entries in \$\textbackslash lbrace\{-1,1\textbackslash rbrace\}\$ with \$\textbackslash |Ax\textbackslash |\_\{\textbackslash infty\} \textbackslash leq O(\textbackslash sqrt\{\textbackslash log n\})\$ in polynomial time, where \$A\$ is any matrix whose columns have length at most 1. We show that our method can be used to deterministically obtain such a vector.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/rushilma/Zotero/storage/RKVT5FNM/Levy et al. - 2017 - Deterministic Discrepancy Minimization via the Multiplicative Weight Update Method.pdf}
}

@online{lovettConstructiveDiscrepancyMinimization2012,
  title = {Constructive {{Discrepancy Minimization}} by {{Walking}} on {{The Edges}}},
  author = {Lovett, Shachar and Meka, Raghu},
  date = {2012-10-11},
  eprint = {1203.5747},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1203.5747},
  url = {http://arxiv.org/abs/1203.5747},
  urldate = {2025-03-16},
  abstract = {Minimizing the discrepancy of a set system is a fundamental problem in combinatorics. One of the cornerstones in this area is the celebrated six standard deviations result of Spencer (AMS 1985): In any system of n sets in a universe of size n, there always exists a coloring which achieves discrepancy 6\textbackslash sqrt\{n\}. The original proof of Spencer was existential in nature, and did not give an efficient algorithm to find such a coloring. Recently, a breakthrough work of Bansal (FOCS 2010) gave an efficient algorithm which finds such a coloring. His algorithm was based on an SDP relaxation of the discrepancy problem and a clever rounding procedure. In this work we give a new randomized algorithm to find a coloring as in Spencer's result based on a restricted random walk we call "Edge-Walk". Our algorithm and its analysis use only basic linear algebra and is "truly" constructive in that it does not appeal to the existential arguments, giving a new proof of Spencer's theorem and the partial coloring lemma.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/rushilma/Zotero/storage/WZI2E3RV/Lovett and Meka - 2012 - Constructive Discrepancy Minimization by Walking on The Edges.pdf;/Users/rushilma/Zotero/storage/5HCRJQMI/1203.html}
}

@article{luekerNoteAveragecaseBehavior1987,
  title = {A Note on the Average-Case Behavior of a Simple Differencing Method for Partitioning},
  author = {Lueker, George S},
  date = {1987-12-01},
  journaltitle = {Operations Research Letters},
  shortjournal = {Operations Research Letters},
  volume = {6},
  number = {6},
  pages = {285--287},
  issn = {0167-6377},
  doi = {10.1016/0167-6377(87)90044-7},
  url = {https://www.sciencedirect.com/science/article/pii/0167637787900447},
  urldate = {2025-03-16},
  abstract = {Given n positive values x1, x2,…, xn, we wish to partition them into two subsets such that the difference between the sums of the subsets is minimized. Karp and Karmarkar have shown that a fairly complicated linear-time differencing algorithm achieves, for a broad class of probability distributions, a difference of O(n−alogn), for some α {$>$} 0, with probability approaching 1 as n → ∞. Their work left open the question of how two simple and more natural implementations of the algorithm behaved. In this paper, under the assumption that the input values are uniformly distributed, we show that one of these algorithms is not nearly so effective, confirming empirical observations of Karp.},
  keywords = {average-case analysis,optimization algorithms,partition problem}
}

@inproceedings{mekaSumofsquaresLowerBounds2015,
  title = {Sum-of-Squares {{Lower Bounds}} for {{Planted Clique}}},
  booktitle = {Proceedings of the Forty-Seventh Annual {{ACM}} Symposium on {{Theory}} of {{Computing}}},
  author = {Meka, Raghu and Potechin, Aaron and Wigderson, Avi},
  date = {2015-06-14},
  pages = {87--96},
  publisher = {ACM},
  location = {Portland Oregon USA},
  doi = {10.1145/2746539.2746600},
  url = {https://dl.acm.org/doi/10.1145/2746539.2746600},
  urldate = {2025-03-16},
  eventtitle = {{{STOC}} '15: {{Symposium}} on {{Theory}} of {{Computing}}},
  isbn = {978-1-4503-3536-2},
  langid = {english},
  file = {/Users/rushilma/Zotero/storage/UJ4YARC8/Meka et al. - 2015 - Sum-of-squares Lower Bounds for Planted Clique.pdf}
}

@article{merkleHidingInformationSignatures1978,
  title = {Hiding Information and Signatures in Trapdoor Knapsacks},
  author = {Merkle, R. and Hellman, M.},
  date = {1978-09},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {24},
  number = {5},
  pages = {525--530},
  issn = {1557-9654},
  doi = {10.1109/TIT.1978.1055927},
  url = {https://ieeexplore.ieee.org/document/1055927},
  urldate = {2025-03-15},
  abstract = {The knapsack problem is an NP-complete combinatorial problem that is strongly believed to be computationally difficult to solve in general. Specific instances of this problem that appear very difficult to solve unless one possesses "trapdoor information" used in the design of the problem are demonstrated. Because only the designer can easily solve problems, others can send him information hidden in the solution to the problems without fear that an eavesdropper will be able to extract the information. This approach differs from usual cryptographic systems in that a secret key is not needed. Conversely, only the designer can generate signatures for messages, but anyone can easily check their authenticity.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  file = {/Users/rushilma/Zotero/storage/9KTRD6GE/1055927.html}
}

@online{mertensEasiestHardProblem2003,
  title = {The {{Easiest Hard Problem}}: {{Number Partitioning}}},
  shorttitle = {The {{Easiest Hard Problem}}},
  author = {Mertens, Stephan},
  date = {2003-10-18},
  eprint = {cond-mat/0310317},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.cond-mat/0310317},
  url = {http://arxiv.org/abs/cond-mat/0310317},
  urldate = {2025-03-15},
  abstract = {Number partitioning is one of the classical NP-hard problems of combinatorial optimization. It has applications in areas like public key encryption and task scheduling. The random version of number partitioning has an "easy-hard" phase transition similar to the phase transitions observed in other combinatorial problems like \$k\$-SAT. In contrast to most other problems, number partitioning is simple enough to obtain detailled and rigorous results on the "hard" and "easy" phase and the transition that separates them. We review the known results on random integer partitioning, give a very simple derivation of the phase transition and discuss the algorithmic implications of both phases.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/rushilma/Zotero/storage/SX9GF6SY/Mertens - 2003 - The Easiest Hard Problem Number Partitioning.pdf;/Users/rushilma/Zotero/storage/FZEGWGUH/0310317.html}
}

@article{mertensPhysicistsApproachNumber2001,
  title = {A Physicist's Approach to Number Partitioning},
  author = {Mertens, Stephan},
  date = {2001-08-28},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Phase {{Transitions}} in {{Combinatorial Problems}}},
  volume = {265},
  number = {1},
  pages = {79--108},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(01)00153-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397501001530},
  urldate = {2025-03-15},
  abstract = {The statistical physics approach to the number partioning problem, a classical NP-hard problem, is both simple and rewarding. Very basic notions and methods from statistical mechanics are enough to obtain analytical results for the phase boundary that separates the “easy-to-solve” from the “hard-to-solve” phase of the NPP as well as for the probability distributions of the optimal and sub-optimal solutions. In addition, it can be shown that solving a number partioning problem of size N to some extent corresponds to locating the minimum in an unsorted list of O(2N) numbers. Considering this correspondence it is not surprising that known heuristics for the partitioning problem are not significantly better than simple random search.},
  keywords = {Heuristic algorithms,NP-complete,Number partitioning,Phase transition,Random cost problem,Statistical mechanics},
  file = {/Users/rushilma/Zotero/storage/4QEJM9H7/Mertens - 2001 - A physicist's approach to number partitioning.pdf}
}

@article{mezardClusteringSolutionsRandom2005,
  title = {Clustering of {{Solutions}} in the {{Random Satisfiability Problem}}},
  author = {Mézard, M. and Mora, T. and Zecchina, R.},
  date = {2005-05-19},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {94},
  number = {19},
  pages = {197205},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.94.197205},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.94.197205},
  urldate = {2025-03-16},
  langid = {english},
  file = {/Users/rushilma/Zotero/storage/4MG3GRBP/Mézard et al. - 2005 - Clustering of Solutions in the Random Satisfiability Problem.pdf}
}

@incollection{michielsPerformanceRatiosDifferencing2003,
  title = {Performance {{Ratios}} for the {{Differencing Method Applied}} to the {{Balanced Number Partitioning Problem}}},
  booktitle = {{{STACS}} 2003},
  author = {Michiels, Wil and Korst, Jan and Aarts, Emile and Van Leeuwen, Jan},
  editor = {Alt, Helmut and Habib, Michel},
  editora = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
  editoratype = {redactor},
  date = {2003},
  volume = {2607},
  pages = {583--595},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-36494-3_51},
  url = {http://link.springer.com/10.1007/3-540-36494-3_51},
  urldate = {2025-03-15},
  isbn = {978-3-540-00623-7 978-3-540-36494-8}
}

@online{montanariEquivalenceApproximateMessage2024,
  title = {Equivalence of {{Approximate Message Passing}} and {{Low-Degree Polynomials}} in {{Rank-One Matrix Estimation}}},
  author = {Montanari, Andrea and Wein, Alexander S.},
  date = {2024-10-10},
  eprint = {2212.06996},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2212.06996},
  url = {http://arxiv.org/abs/2212.06996},
  urldate = {2025-03-26},
  abstract = {We consider the problem of estimating an unknown parameter vector \$\{\textbackslash boldsymbol \textbackslash theta\}\textbackslash in\{\textbackslash mathbb R\}\textasciicircum n\$, given noisy observations \$\{\textbackslash boldsymbol Y\} = \{\textbackslash boldsymbol \textbackslash theta\}\{\textbackslash boldsymbol \textbackslash theta\}\textasciicircum\{\textbackslash top\}/\textbackslash sqrt\{n\}+\{\textbackslash boldsymbol Z\}\$ of the rank-one matrix \$\{\textbackslash boldsymbol \textbackslash theta\}\{\textbackslash boldsymbol \textbackslash theta\}\textasciicircum\{\textbackslash top\}\$, where \$\{\textbackslash boldsymbol Z\}\$ has independent Gaussian entries. When information is available about the distribution of the entries of \$\{\textbackslash boldsymbol \textbackslash theta\}\$, spectral methods are known to be strictly sub-optimal. Past work characterized the asymptotics of the accuracy achieved by the optimal estimator. However, no polynomial-time estimator is known that achieves this accuracy. It has been conjectured that this statistical-computation gap is fundamental, and moreover that the optimal accuracy achievable by polynomial-time estimators coincides with the accuracy achieved by certain approximate message passing (AMP) algorithms. We provide evidence towards this conjecture by proving that no estimator in the (broader) class of constant-degree polynomials can surpass AMP.},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/WSJBJGES/2212.html}
}

@online{montanariOptimizationSherringtonKirkpatrickHamiltonian2019,
  title = {Optimization of the {{Sherrington-Kirkpatrick Hamiltonian}}},
  author = {Montanari, Andrea},
  date = {2019-04-05},
  eprint = {1812.10897},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1812.10897},
  url = {http://arxiv.org/abs/1812.10897},
  urldate = {2025-03-29},
  abstract = {Let \$\{\textbackslash boldsymbol A\}\textbackslash in\{\textbackslash mathbb R\}\textasciicircum\{n\textbackslash times n\}\$ be a symmetric random matrix with independent and identically distributed Gaussian entries above the diagonal. We consider the problem of maximizing \$\textbackslash langle\{\textbackslash boldsymbol \textbackslash sigma\},\{\textbackslash boldsymbol A\}\{\textbackslash boldsymbol \textbackslash sigma\}\textbackslash rangle\$ over binary vectors \$\{\textbackslash boldsymbol \textbackslash sigma\}\textbackslash in\textbackslash\{+1,-1\textbackslash\}\textasciicircum n\$. In the language of statistical physics, this amounts to finding the ground state of the Sherrington-Kirkpatrick model of spin glasses. The asymptotic value of this optimization problem was characterized by Parisi via a celebrated variational principle, subsequently proved by Talagrand. We give an algorithm that, for any \$\textbackslash varepsilon{$>$}0\$, outputs \$\{\textbackslash boldsymbol \textbackslash sigma\}\_*\textbackslash in\textbackslash\{-1,+1\textbackslash\}\textasciicircum n\$ such that \$\textbackslash langle\{\textbackslash boldsymbol \textbackslash sigma\}\_*,\{\textbackslash boldsymbol A\}\{\textbackslash boldsymbol \textbackslash sigma\}\_*\textbackslash rangle\$ is at least \$(1-\textbackslash varepsilon)\$ of the optimum value, with probability converging to one as \$n\textbackslash to\textbackslash infty\$. The algorithm's time complexity is \$C(\textbackslash varepsilon)\textbackslash, n\textasciicircum 2\$. It is a message-passing algorithm, but the specific structure of its update rules is new. As a side result, we prove that, at (low) non-zero temperature, the algorithm constructs approximate solutions of the Thouless-Anderson-Palmer equations.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematics - Optimization and Control,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/X8UKEDRT/Montanari - 2019 - Optimization of the Sherrington-Kirkpatrick Hamiltonian.pdf;/Users/rushilma/Zotero/storage/E6MAPYQ3/1812.html}
}

@online{odonnellAnalysisBooleanFunctions2021,
  title = {Analysis of {{Boolean Functions}}},
  author = {O'Donnell, Ryan},
  date = {2021-05-21},
  eprint = {2105.10386},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2105.10386},
  url = {http://arxiv.org/abs/2105.10386},
  urldate = {2025-03-15},
  abstract = {The subject of this textbook is the analysis of Boolean functions. Roughly speaking, this refers to studying Boolean functions \$f : \textbackslash\{0,1\textbackslash\}\textasciicircum n \textbackslash to \textbackslash\{0,1\textbackslash\}\$ via their Fourier expansion and other analytic means. Boolean functions are perhaps the most basic object of study in theoretical computer science, and Fourier analysis has become an indispensable tool in the field. The topic has also played a key role in several other areas of mathematics, from combinatorics, random graph theory, and statistical physics, to Gaussian geometry, metric/Banach spaces, and social choice theory. The intent of this book is both to develop the foundations of the field and to give a wide (though far from exhaustive) overview of its applications. Each chapter ends with a "highlight" showing the power of analysis of Boolean functions in different subject areas: property testing, social choice, cryptography, circuit complexity, learning theory, pseudorandomness, hardness of approximation, concrete complexity, and random graph theory. The book can be used as a reference for working researchers or as the basis of a one-semester graduate-level course. The author has twice taught such a course at Carnegie Mellon University, attended mainly by graduate students in computer science and mathematics but also by advanced undergraduates, postdocs, and researchers in adjacent fields. In both years most of Chapters 1-5 and 7 were covered, along with parts of Chapters 6, 8, 9, and 11, and some additional material on additive combinatorics. Nearly 500 exercises are provided at the ends of the book's chapters.},
  pubstate = {prepublished},
  keywords = {Computer Science - Discrete Mathematics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/8HBEY6XQ/O'Donnell - 2021 - Analysis of Boolean Functions.pdf;/Users/rushilma/Zotero/storage/E982KKBZ/2105.html}
}

@online{raghavendraHighdimensionalEstimationSumofsquares2019,
  title = {High-Dimensional Estimation via Sum-of-Squares Proofs},
  author = {Raghavendra, Prasad and Schramm, Tselil and Steurer, David},
  date = {2019-08-06},
  eprint = {1807.11419},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1807.11419},
  url = {http://arxiv.org/abs/1807.11419},
  urldate = {2025-03-16},
  abstract = {Estimation is the computational task of recovering a hidden parameter \$x\$ associated with a distribution \$D\_x\$, given a measurement \$y\$ sampled from the distribution. High dimensional estimation problems arise naturally in statistics, machine learning, and complexity theory. Many high dimensional estimation problems can be formulated as systems of polynomial equations and inequalities, and thus give rise to natural probability distributions over polynomial systems. Sum-of-squares proofs provide a powerful framework to reason about polynomial systems, and further there exist efficient algorithms to search for low-degree sum-of-squares proofs. Understanding and characterizing the power of sum-of-squares proofs for estimation problems has been a subject of intense study in recent years. On one hand, there is a growing body of work utilizing sum-of-squares proofs for recovering solutions to polynomial systems when the system is feasible. On the other hand, a general technique referred to as pseudocalibration has been developed towards showing lower bounds on the degree of sum-of-squares proofs. Finally, the existence of sum-of-squares refutations of a polynomial system has been shown to be intimately connected to the existence of spectral algorithms. In this article we survey these developments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/7R23WFHB/Raghavendra et al. - 2019 - High-dimensional estimation via sum-of-squares proofs.pdf;/Users/rushilma/Zotero/storage/YXBQMIGY/1807.html}
}

@article{rahmanLocalAlgorithmsIndependent2017,
  title = {Local Algorithms for Independent Sets Are Half-Optimal},
  author = {Rahman, Mustazee and Virag, Balint},
  date = {2017-05-01},
  journaltitle = {The Annals of Probability},
  shortjournal = {Ann. Probab.},
  volume = {45},
  number = {3},
  eprint = {1402.0485},
  eprinttype = {arXiv},
  eprintclass = {math},
  issn = {0091-1798},
  doi = {10.1214/16-AOP1094},
  url = {http://arxiv.org/abs/1402.0485},
  urldate = {2025-03-16},
  abstract = {We show that the largest density of factor of i.i.d. independent sets on the d-regular tree is asymptotically at most (log d)/d as d tends to infinity. This matches the lower bound given by previous constructions. It follows that the largest independent sets given by local algorithms on random d-regular graphs have the same asymptotic density. In contrast, the density of the largest independent sets on these graphs is asymptotically 2(log d)/d. We also prove analogous results for Poisson-Galton-Watson trees, which yield bounds for local algorithms on sparse Erdos-Renyi graphs.},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Mathematics - Combinatorics,Mathematics - Probability},
  file = {/Users/rushilma/Zotero/storage/3UI53X83/Rahman and Virag - 2017 - Local algorithms for independent sets are half-optimal.pdf;/Users/rushilma/Zotero/storage/N264JUQV/1402.html}
}

@online{rothvossConstructiveDiscrepancyMinimization2016,
  title = {Constructive Discrepancy Minimization for Convex Sets},
  author = {Rothvoss, Thomas},
  date = {2016-04-12},
  eprint = {1404.0339},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1404.0339},
  url = {http://arxiv.org/abs/1404.0339},
  urldate = {2025-03-16},
  abstract = {A classical theorem of Spencer shows that any set system with n sets and n elements admits a coloring of discrepancy O(n\textasciicircum 1/2). Recent exciting work of Bansal, Lovett and Meka shows that such colorings can be found in polynomial time. In fact, the Lovett-Meka algorithm finds a half integral point in any "large enough" polytope. However, their algorithm crucially relies on the facet structure and does not apply to general convex sets. We show that for any symmetric convex set K with measure at least exp(-n/500), the following algorithm finds a point y in K \textbackslash cap [-1,1]\textasciicircum n with Omega(n) coordinates in \{-1,+1\}: (1) take a random Gaussian vector x; (2) compute the point y in K \textbackslash cap [-1,1]\textasciicircum n that is closest to x. (3) return y. This provides another truly constructive proof of Spencer's theorem and the first constructive proof of a Theorem of Gluskin and Giannopoulos.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/rushilma/Zotero/storage/DUDWBMFU/Rothvoss - 2016 - Constructive discrepancy minimization for convex sets.pdf;/Users/rushilma/Zotero/storage/PR462NBP/1404.html}
}

@article{santucciImprovedMemeticAlgebraic2021,
  title = {An Improved Memetic Algebraic Differential Evolution for Solving the Multidimensional Two-Way Number Partitioning Problem},
  author = {Santucci, Valentino and Baioletti, Marco and Di Bari, Gabriele},
  date = {2021-09-15},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {178},
  pages = {114938},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.114938},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417421003791},
  urldate = {2025-03-15},
  abstract = {In this article, we propose a novel and effective evolutionary algorithm for the challenging combinatorial optimization problem known as Multidimensional Two-Way Number Partitioning Problem (MDTWNPP). Since the MDTWNPP has been proven to be NP-hard, in the recent years, it has been increasingly addressed by means of meta-heuristic approaches. Nevertheless, previous proposals in literature do not make full use of critical problem information that may improve the effectiveness of the search. Here, we bridge this gap by designing an improved Memetic Algebraic Differential Evolution (iMADEB) algorithm that incorporates critical information about the problem. In particular, iMADEB evolves a population of candidate local optimal solutions by adopting three key design concepts: a novel non-redundant bit-string representation which maps population individuals one-to-one to MDTWNPP solutions, a smoother local search operator purposely designed for the MDTWNPP landscapes, and a self-adaptive algebraic differential mutation scheme built on the basis of the Lévy flight concept which automatically regulates the exploration-exploitation trade-off of the search. Computational experiments have been conducted on a widely accepted benchmark suite for the MDTWNPP with a twofold purpose: analyzing the robustness of iMADEB and compare its effectiveness with respect to the state-of-the-art approaches to date for the MDTWNPP. The experimental results provide important indications about iMADEB robustness and, most importantly, clearly show that iMADEB is the new state-of-the-art algorithm for the MDTWNPP.},
  keywords = {Algebraic Differential Evolution,Combinatorial optimization,Memetic Algorithm,Multidimensional Two-Way Number Partitioning},
  file = {/Users/rushilma/Zotero/storage/QNRLN3I9/S0957417421003791.html}
}

@inproceedings{shamirPolynomialTimeAlgorithm1982,
  title = {A Polynomial Time Algorithm for Breaking the Basic {{Merkle-Hellman}} Cryptosystem},
  booktitle = {23rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1982)},
  author = {Shamir, Adi},
  date = {1982-11},
  pages = {145--152},
  issn = {0272-5428},
  doi = {10.1109/SFCS.1982.5},
  url = {https://ieeexplore.ieee.org/document/4568386},
  urldate = {2025-03-29},
  abstract = {The cryptographic security of the Merkle-Hellman cryptosystem has been a major open problem since 1976. In this paper we show that the basic variant of this cryptosystem, in which the elements of the public key are modular multiples of a superincreasing sequence, is breakable in polynomial time.},
  eventtitle = {23rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1982)},
  keywords = {Communication channels,Greedy algorithms,H infinity control,Mathematics,Performance analysis,Polynomials,Protection,Public key,Public key cryptography,Security},
  file = {/Users/rushilma/Zotero/storage/ZYV97387/4568386.html}
}

@article{storerProblemSpaceLocal1996,
  title = {Problem Space Local Search for Number Partitioning},
  author = {Storer, Robert H. and Flanders, Seth W. and David Wu, S.},
  date = {1996-08-01},
  journaltitle = {Annals of Operations Research},
  shortjournal = {Ann Oper Res},
  volume = {63},
  number = {4},
  pages = {463--487},
  issn = {1572-9338},
  doi = {10.1007/BF02156630},
  url = {https://doi.org/10.1007/BF02156630},
  urldate = {2025-03-15},
  abstract = {We show how simple and effective metaheuristics can be developed for the number partitioning problem using the “problem space” approach [1]. In a previous application of local search to number partitioning [2], it was found that the performance of simulated annealing used in conjunction with “swap neighborhoods” was disappointing relative to the differencing heuristic of Karmarkar and Karp [3]. Using problem space neighborhoods as an alternative to swapping, we empirically demonstrate several orders of magnitude improvement over the differencing algorithm, albeit with greater computation time. This improvement in performance comes as little surprise since a modified version of the differencing heuristic is explicitly embedded in the problem space algorithm.},
  langid = {english},
  keywords = {genetic algorithms,local search,Number partitioning}
}

@article{tsaiAsymptoticAnalysisAlgorithm1992,
  title = {Asymptotic {{Analysis}} of an {{Algorithm}} for {{Balanced Parallel Processor Scheduling}}},
  author = {Tsai, Li-Hui},
  date = {1992-02},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  volume = {21},
  number = {1},
  pages = {59--64},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/0221007},
  url = {https://epubs.siam.org/doi/abs/10.1137/0221007},
  urldate = {2025-03-15},
  abstract = {This investigation considers the problem of nonpreemptively assigning a set of independent tasks to a system of identical processors to maximize the earliest processor finishing time. While this goal is a nonstandard scheduling criterion, it does have natural applications in certain maintenance scheduling and deterministic fleet sizing problems. The problem is NP-hard, justifying an analysis of heuristics such as the well-known LPT algorithm in an effort to guarantee near-optimal results. It is proved that the worst-case performance of the LPT algorithm has an asymptotically tight bound of \$\textbackslash frac\{4\}\{3\}\$ times the optimal.}
}

@online{turnerBalancingGaussianVectors2020,
  title = {Balancing {{Gaussian}} Vectors in High Dimension},
  author = {Turner, Paxton and Meka, Raghu and Rigollet, Philippe},
  date = {2020-06-30},
  eprint = {1910.13972},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.13972},
  url = {http://arxiv.org/abs/1910.13972},
  urldate = {2025-03-16},
  abstract = {Motivated by problems in controlled experiments, we study the discrepancy of random matrices with continuous entries where the number of columns \$n\$ is much larger than the number of rows \$m\$. Our first result shows that if \$\textbackslash omega(1) = m = o(n)\$, a matrix with i.i.d. standard Gaussian entries has discrepancy \$\textbackslash Theta(\textbackslash sqrt\{n\} \textbackslash, 2\textasciicircum\{-n/m\})\$ with high probability. This provides sharp guarantees for Gaussian discrepancy in a regime that had not been considered before in the existing literature. Our results also apply to a more general family of random matrices with continuous i.i.d entries, assuming that \$m = O(n/\textbackslash log\{n\})\$. The proof is non-constructive and is an application of the second moment method. Our second result is algorithmic and applies to random matrices whose entries are i.i.d. and have a Lipschitz density. We present a randomized polynomial-time algorithm that achieves discrepancy \$e\textasciicircum\{-\textbackslash Omega(\textbackslash log\textasciicircum 2(n)/m)\}\$ with high probability, provided that \$m = O(\textbackslash sqrt\{\textbackslash log\{n\}\})\$. In the one-dimensional case, this matches the best known algorithmic guarantees due to Karmarkar--Karp. For higher dimensions \$2 \textbackslash leq m = O(\textbackslash sqrt\{\textbackslash log\{n\}\})\$, this establishes the first efficient algorithm achieving discrepancy smaller than \$O( \textbackslash sqrt\{m\} )\$.},
  pubstate = {prepublished},
  keywords = {Computer Science - Discrete Mathematics,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/AG27L93P/Turner et al. - 2020 - Balancing Gaussian vectors in high dimension.pdf;/Users/rushilma/Zotero/storage/NA6HQUKV/1910.html}
}

@online{vafaSymmetricPerceptronsNumber2025,
  title = {Symmetric {{Perceptrons}}, {{Number Partitioning}} and {{Lattices}}},
  author = {Vafa, Neekon and Vaikuntanathan, Vinod},
  date = {2025-01-27},
  eprint = {2501.16517},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2501.16517},
  url = {http://arxiv.org/abs/2501.16517},
  urldate = {2025-03-20},
  abstract = {The symmetric binary perceptron (\$\textbackslash mathrm\{SBP\}\_\{\textbackslash kappa\}\$) problem with parameter \$\textbackslash kappa : \textbackslash mathbb\{R\}\_\{\textbackslash geq1\} \textbackslash to [0,1]\$ is an average-case search problem defined as follows: given a random Gaussian matrix \$\textbackslash mathbf\{A\} \textbackslash sim \textbackslash mathcal\{N\}(0,1)\textasciicircum\{n \textbackslash times m\}\$ as input where \$m \textbackslash geq n\$, output a vector \$\textbackslash mathbf\{x\} \textbackslash in \textbackslash\{-1,1\textbackslash\}\textasciicircum m\$ such that \$\$|| \textbackslash mathbf\{A\} \textbackslash mathbf\{x\} ||\_\{\textbackslash infty\} \textbackslash leq \textbackslash kappa(m/n) \textbackslash cdot \textbackslash sqrt\{m\}\textasciitilde.\$\$ The number partitioning problem (\$\textbackslash mathrm\{NPP\}\_\{\textbackslash kappa\}\$) corresponds to the special case of setting \$n=1\$. There is considerable evidence that both problems exhibit large computational-statistical gaps. In this work, we show (nearly) tight average-case hardness for these problems, assuming the worst-case hardness of standard approximate shortest vector problems on lattices. For \$\textbackslash mathrm\{SBP\}\$, for large \$n\$, the best that efficient algorithms have been able to achieve is \$\textbackslash kappa(x) = \textbackslash Theta(1/\textbackslash sqrt\{x\})\$ (Bansal and Spencer, Random Structures and Algorithms 2020), which is a far cry from the statistical bound. The problem has been extensively studied in the TCS and statistics communities, and Gamarnik, Kizildag, Perkins and Xu (FOCS 2022) conjecture that Bansal-Spencer is tight: namely, \$\textbackslash kappa(x) = \textbackslash widetilde\{\textbackslash Theta\}(1/\textbackslash sqrt\{x\})\$ is the optimal value achieved by computationally efficient algorithms. We prove their conjecture assuming the worst-case hardness of approximating the shortest vector problem on lattices. For \$\textbackslash mathrm\{NPP\}\$, Karmarkar and Karp's classical differencing algorithm achieves \$\textbackslash kappa(m) = 2\textasciicircum\{-O(\textbackslash log\textasciicircum 2 m)\}\textasciitilde.\$ We prove that Karmarkar-Karp is nearly tight: namely, no polynomial-time algorithm can achieve \$\textbackslash kappa(m) = 2\textasciicircum\{-\textbackslash Omega(\textbackslash log\textasciicircum 3 m)\}\$, once again assuming the worst-case subexponential hardness of approximating the shortest vector problem on lattices to within a subexponential factor.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Mathematical Physics,Mathematics - Mathematical Physics,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/rushilma/Zotero/storage/NNFLPB9Z/Vafa and Vaikuntanathan - 2025 - Symmetric Perceptrons, Number Partitioning and Lattices.pdf;/Users/rushilma/Zotero/storage/WM659VB8/2501.html}
}

@book{vershyninHighDimensionalProbabilityIntroduction2018,
  title = {High-{{Dimensional Probability}}: {{An Introduction}} with {{Applications}} in {{Data Science}}},
  shorttitle = {High-{{Dimensional Probability}}},
  author = {Vershynin, Roman},
  date = {2018},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  edition = {1st ed},
  publisher = {Cambridge University Press},
  location = {New York, NY},
  isbn = {978-1-108-41519-4 978-1-108-23159-6},
  langid = {english},
  pagetotal = {1}
}

@book{wainwrightHighDimensionalStatisticsNonAsymptotic2019,
  title = {High-{{Dimensional Statistics}}: {{A Non-Asymptotic Viewpoint}}},
  shorttitle = {High-{{Dimensional Statistics}}},
  author = {Wainwright, Martin J.},
  date = {2019},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/9781108627771},
  url = {https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E},
  urldate = {2024-12-28},
  abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
  isbn = {978-1-108-49802-9},
  file = {/Users/rushilma/Zotero/storage/E4LYV3IK/8A91ECEEC38F46DAB53E9FF8757C7A4E.html}
}

@online{weinOptimalLowDegreeHardness2020,
  title = {Optimal {{Low-Degree Hardness}} of {{Maximum Independent Set}}},
  author = {Wein, Alexander S.},
  date = {2020-11-12},
  eprint = {2010.06563},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.06563},
  url = {http://arxiv.org/abs/2010.06563},
  urldate = {2025-03-16},
  abstract = {We study the algorithmic task of finding a large independent set in a sparse Erd\textbackslash H\{o\}s-R\textbackslash '\{e\}nyi random graph with \$n\$ vertices and average degree \$d\$. The maximum independent set is known to have size \$(2 \textbackslash log d / d)n\$ in the double limit \$n \textbackslash to \textbackslash infty\$ followed by \$d \textbackslash to \textbackslash infty\$, but the best known polynomial-time algorithms can only find an independent set of half-optimal size \$(\textbackslash log d / d)n\$. We show that the class of low-degree polynomial algorithms can find independent sets of half-optimal size but no larger, improving upon a result of Gamarnik, Jagannath, and the author. This generalizes earlier work by Rahman and Vir\textbackslash 'ag, which proved the analogous result for the weaker class of local algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/LTXRCAU4/Wein - 2020 - Optimal Low-Degree Hardness of Maximum Independent Set.pdf;/Users/rushilma/Zotero/storage/R6XF8NEX/2010.html}
}

@article{wenOpticalExperimentalSolution2023,
  title = {Optical Experimental Solution for the Multiway Number Partitioning Problem and Its Application to Computing Power Scheduling},
  author = {Wen, Jingwei and Wang, Zhenming and Huang, Zhiguo and Cai, Dunbo and Jia, Bingjie and Cao, Chongyu and Ma, Yin and Wei, Hai and Wen, Kai and Qian, Ling},
  date = {2023-09},
  journaltitle = {Science China Physics, Mechanics \& Astronomy},
  shortjournal = {Sci. China Phys. Mech. Astron.},
  volume = {66},
  number = {9},
  pages = {290313},
  issn = {1674-7348, 1869-1927},
  doi = {10.1007/s11433-023-2147-3},
  url = {https://link.springer.com/10.1007/s11433-023-2147-3},
  urldate = {2025-03-15},
  langid = {english}
}

@article{yakirDifferencingAlgorithmLDM1996,
  title = {The {{Differencing Algorithm LDM}} for {{Partitioning}}: {{A Proof}} of a {{Conjecture}} of {{Karmarkar}} and {{Karp}}},
  shorttitle = {The {{Differencing Algorithm LDM}} for {{Partitioning}}},
  author = {Yakir, Benjamin},
  date = {1996-02},
  journaltitle = {Mathematics of Operations Research},
  shortjournal = {Mathematics of OR},
  volume = {21},
  number = {1},
  pages = {85--99},
  publisher = {INFORMS},
  issn = {0364-765X},
  doi = {10.1287/moor.21.1.85},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/moor.21.1.85},
  urldate = {2025-03-15},
  abstract = {The algorithm LDM (largest differencing method) divides a list of n random items into two blocks. The parameter of interest is the expected difference between the two block sums. It is shown that if the items are i.i.d. and uniform then the rate of convergence of this parameter to zero is n−Θ(log n). An algorithm for balanced partitioning is constructed, with the same rate of convergence to zero.},
  keywords = {differencing method,makespan,partitioning,probabilistic analysis of algorithms,scheduling}
}

@article{zdeborovaStatisticalPhysicsInference2016,
  title = {Statistical Physics of Inference: {{Thresholds}} and Algorithms},
  shorttitle = {Statistical Physics of Inference},
  author = {Zdeborová, Lenka and Krzakala, Florent},
  date = {2016-09-02},
  journaltitle = {Advances in Physics},
  shortjournal = {Advances in Physics},
  volume = {65},
  number = {5},
  eprint = {1511.02476},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  pages = {453--552},
  issn = {0001-8732, 1460-6976},
  doi = {10.1080/00018732.2016.1211393},
  url = {http://arxiv.org/abs/1511.02476},
  urldate = {2025-03-16},
  abstract = {Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic.},
  keywords = {Computer Science - Data Structures and Algorithms,Condensed Matter - Statistical Mechanics,Statistics - Machine Learning},
  file = {/Users/rushilma/Zotero/storage/LYI5MX5M/Zdeborová and Krzakala - 2016 - Statistical physics of inference Thresholds and algorithms.pdf;/Users/rushilma/Zotero/storage/MCT2PFLH/1511.html}
}
